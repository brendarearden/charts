diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/Chart.yaml packages/rancher-monitoring/charts/Chart.yaml
--- packages/rancher-monitoring/charts-original/Chart.yaml
+++ packages/rancher-monitoring/charts/Chart.yaml
@@ -1,22 +1,28 @@
 apiVersion: v1
 appVersion: 0.38.1
-description: Provides easy monitoring definitions for Kubernetes services, and deployment
-  and management of Prometheus instances.
+annotations:
+  catalog.cattle.io/certified: rancher
+  catalog.cattle.io/namespace: cattle-monitoring-system
+  catalog.cattle.io/release-name: rancher-monitoring
+  catalog.cattle.io/ui-component: monitoring
+  catalog.cattle.io/provides-gvr: monitoring.coreos.com.prometheus/v1
+description: Modifies the upstream Prometheus Operator chart, which provides easy monitoring definitions for Kubernetes services and the deployment and management of Prometheus instances, and enables Prometheus Adapter on a default Prometheus instance.
 engine: gotpl
 home: https://github.com/coreos/prometheus-operator
 icon: https://raw.githubusercontent.com/prometheus/prometheus.github.io/master/assets/prometheus_logo-cb55bb5c346.png
 keywords:
-- operator
-- prometheus
+  - operator
+  - prometheus
+  - monitoring
 maintainers:
-- name: vsliouniaev
-- name: bismarck
-- email: gianrubio@gmail.com
-  name: gianrubio
-name: prometheus-operator
+  - name: vsliouniaev
+  - name: bismarck
+  - email: gianrubio@gmail.com
+    name: gianrubio
+name: rancher-monitoring
 sources:
-- https://github.com/coreos/kube-prometheus
-- https://github.com/coreos/prometheus-operator
-- https://coreos.com/operators/prometheus
+  - https://github.com/coreos/kube-prometheus
+  - https://github.com/coreos/prometheus-operator
+  - https://coreos.com/operators/prometheus
 tillerVersion: '>=2.12.0'
 version: 8.16.1
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/README.md packages/rancher-monitoring/charts/README.md
--- packages/rancher-monitoring/charts-original/README.md
+++ packages/rancher-monitoring/charts/README.md
@@ -2,6 +2,8 @@
 
 Installs [prometheus-operator](https://github.com/coreos/prometheus-operator) to create/configure/manage Prometheus clusters atop Kubernetes. This chart includes multiple components and is suitable for a variety of use-cases.
 
+You must install the Prometheus Operator CRDs first using the `rancher-monitoring-crd` chart before installing this chart.
+
 The default installation is intended to suit monitoring a kubernetes cluster the chart is deployed onto. It closely matches the kube-prometheus project.
 - [prometheus-operator](https://github.com/coreos/prometheus-operator)
 - [prometheus](https://prometheus.io/)
@@ -9,6 +11,12 @@
 - [node-exporter](https://github.com/helm/charts/tree/master/stable/prometheus-node-exporter)
 - [kube-state-metrics](https://github.com/helm/charts/tree/master/stable/kube-state-metrics)
 - [grafana](https://github.com/helm/charts/tree/master/stable/grafana)
+- [prometheus-adapter](https://github.com/helm/charts/tree/master/stable/prometheus-adapter)
+- [rancher-pushprox](https://github.com/rancher/dev-charts/tree/master/packages/rancher-pushprox) charts to monitor internal kubernetes components for k3s, rke, and kubeAdm clusters
+  - kube-scheduler
+  - kube-controller-manager
+  - kube-proxy
+  - kube-etcd (only rke and kubeAdm)
 - service monitors to scrape internal kubernetes components
   - kube-apiserver
   - kube-scheduler
@@ -136,6 +144,30 @@
 
 The following tables list the configurable parameters of the prometheus-operator chart and their default values.
 
+### Rancher Monitoring Configuration
+| Parameter | Description | Default |
+| ----- | ----------- | ------ |
+| `prometheus-adapter.enabled` | Whether to install [prometheus-adapter](https://github.com/helm/charts/tree/master/stable/prometheus-adapter) within the cluster | `true` |
+| `prometheus-adapter.prometheus.url` | A URL pointing to the Prometheus deployment within your cluster. The default value is set based on the assumption that you plan to deploy the default Prometheus instance from this chart where `.Values.namespaceOverride=cattle-monitoring-system` and `.Values.nameOverride=rancher-monitoring` | `http://rancher-monitoring-prometheus.cattle-monitoring-system.svc` |
+| `prometheus-adapter.prometheus.port` | The port on the Prometheus deployment that Prometheus Adapter can make requests to | `9090` |
+
+The following values are enabled for different distributions via [rancher-pushprox](https://github.com/rancher/dev-charts/tree/master/packages/rancher-pushprox). See the rancher-pushprox `README.md` for more information on what all values can be configured for the PushProxy chart.
+
+| Parameter | Description | Default |
+| ----- | ----------- | ------ |
+| `rkeControllerManager.enabled` | Create a PushProx installation for monitoring kube-controller-manager metrics in RKE clusters | `false` |
+| `rkeScheduler.enabled` | Create a PushProx installation for monitoring kube-scheduler metrics in RKE clusters | `false` |
+| `rkeProxy.enabled` | Create a PushProx installation for monitoring kube-proxy metrics in RKE clusters | `false` |
+| `rkeEtcd.enabled` | Create a PushProx installation for monitoring etcd metrics in RKE clusters | `false` |
+| `k3sControllerManager.enabled` | Create a PushProx installation for monitoring kube-controller-manager metrics in k3s clusters | `false` |
+| `k3sScheduler.enabled` | Create a PushProx installation for monitoring kube-scheduler metrics in k3s clusters | `false` |
+| `k3sProxy.enabled` | Create a PushProx installation for monitoring kube-proxy metrics in k3s clusters | `false` |
+| `kubeAdmControllerManager.enabled` | Create a PushProx installation for monitoring kube-controller-manager metrics in kubeAdm clusters | `false` |
+| `kubeAdmScheduler.enabled` | Create a PushProx installation for monitoring kube-scheduler metrics in kubeAdm clusters | `false` |
+| `kubeAdmProxy.enabled` | Create a PushProx installation for monitoring kube-proxy metrics in kubeAdm clusters | `false` |
+| `kubeAdmEtcd.enabled` | Create a PushProx installation for monitoring etcd metrics in kubeAdm clusters | `false` |
+
+
 ### General
 | Parameter | Description | Default |
 | ----- | ----------- | ------ |
@@ -173,7 +205,9 @@
 | `defaultRules.rules.time` | Create time default rules | `true` |
 | `fullnameOverride` | Provide a name to substitute for the full names of resources |`""`|
 | `global.imagePullSecrets` | Reference to one or more secrets to be used when pulling images | `[]` |
-| `global.rbac.create` | Create RBAC resources | `true` |
+| `global.rbac.create` | Create RBAC resources for ServiceAccounts and users | `true` |
+| `global.rbac.userRoles.create` | Create default user ClusterRoles to allow users to interact with Prometheus CRs, ConfigMaps, and Secrets | `true` |
+| `global.rbac.userRoles.aggregateToDefaultRoles` | Aggregate default user ClusterRoles into default k8s ClusterRoles | `true` |
 | `global.rbac.pspEnabled` | Create pod security policy resources | `true` |
 | `global.rbac.pspAnnotations` | Add annotations to the PSP configurations | `{}` |
 | `kubeTargetVersionOverride` | Provide a target gitVersion of K8S, in case .Capabilites.KubeVersion is not available (e.g. `helm template`) |`""`|
@@ -188,30 +222,25 @@
 | `prometheusOperator.admissionWebhooks.failurePolicy` | Failure policy for admission webhooks | `Fail` |
 | `prometheusOperator.admissionWebhooks.patch.enabled` | If true, will use a pre and post install hooks to generate a CA and certificate to use for the prometheus operator tls proxy, and patch the created webhooks with the CA. | `true` |
 | `prometheusOperator.admissionWebhooks.patch.image.pullPolicy` | Image pull policy for the webhook integration jobs | `IfNotPresent` |
-| `prometheusOperator.admissionWebhooks.patch.image.repository` | Repository to use for the webhook integration jobs | `jettech/kube-webhook-certgen` |
+| `prometheusOperator.admissionWebhooks.patch.image.repository` | Repository to use for the webhook integration jobs | `rancher/jettech-kube-webhook-certgen` |
 | `prometheusOperator.admissionWebhooks.patch.image.tag` | Tag to use for the webhook integration jobs | `v1.2.1` |
 | `prometheusOperator.admissionWebhooks.patch.resources` | Resource limits for admission webhook | `{}` |
 | `prometheusOperator.admissionWebhooks.patch.nodeSelector` | Node selector for running admission hook patch jobs | `nil` |
 | `prometheusOperator.admissionWebhooks.patch.podAnnotations` | Annotations for the webhook job pods | `nil` |
 | `prometheusOperator.admissionWebhooks.patch.priorityClassName` | Priority class for the webhook integration jobs | `nil` |
 | `prometheusOperator.affinity` | Assign custom affinity rules to the prometheus operator https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ | `{}` |
-| `prometheusOperator.cleanupCustomResource` | Attempt to delete CRDs when the release is removed. This option may be useful while testing but is not recommended, as deleting the CRD definition will delete resources and prevent the operator from being able to clean up resources that it manages | `false` |
 | `prometheusOperator.configReloaderCpu` | Set the prometheus config reloader side-car CPU limit. If unset, uses the prometheus-operator project default | `nil` |
 | `prometheusOperator.configReloaderMemory` | Set the prometheus config reloader side-car memory limit. If unset, uses the prometheus-operator project default | `nil` |
-| `prometheusOperator.configmapReloadImage.repository` | Repository for configmapReload image | `docker.io/jimmidyson/configmap-reload` |
+| `prometheusOperator.configmapReloadImage.repository` | Repository for configmapReload image | `rancher/jimmidyson-configmap-reload` |
 | `prometheusOperator.configmapReloadImage.tag` | Tag for configmapReload image | `v0.3.0` |
-| `prometheusOperator.createCustomResource` | Create CRDs. Required if deploying anything besides the operator itself as part of the release. The operator will create / update these on startup. If your Helm version < 2.10 you will have to either create the CRDs first or deploy the operator first, then the rest of the resources. Regardless of value of this, Helm v3+ will install the CRDs if those are not present already. Use `--skip-crds` with `helm install` if you want to skip CRD creation | `true` |
 | `prometheusOperator.namespaces` |  Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list). This is mutually exclusive with `denyNamespaces`. Setting this to an empty object will disable the configuration | `{}` |
 | `prometheusOperator.namespaces.releaseNamespace` | Include the release namespace | `false` |
 | `prometheusOperator.namespaces.additional` | Include additional namespaces besides the release namespace | `[]` |
-| `prometheusOperator.manageCrds` |If true prometheus operator will create and update its CRDs on startup (for operator `<v0.39.0`)) | `true` |
+| `prometheusOperator.manageCrds` |If true prometheus operator will create and update its CRs on startup (for operator `<v0.39.0`)) | `true` |
 | `prometheusOperator.denyNamespaces` | Namespaces not to scope the interaction of the Prometheus Operator (deny list). This is mutually exclusive with `namespaces` | `[]` |
 | `prometheusOperator.enabled` | Deploy Prometheus Operator. Only one of these should be deployed into the cluster | `true` |
-| `prometheusOperator.hyperkubeImage.pullPolicy` | Image pull policy for hyperkube image used to perform maintenance tasks | `IfNotPresent` |
-| `prometheusOperator.hyperkubeImage.repository` | Repository for hyperkube image used to perform maintenance tasks | `k8s.gcr.io/hyperkube` |
-| `prometheusOperator.hyperkubeImage.tag` | Tag for hyperkube image used to perform maintenance tasks | `v1.16.12` |
 | `prometheusOperator.image.pullPolicy` | Pull policy for prometheus operator image | `IfNotPresent` |
-| `prometheusOperator.image.repository` | Repository for prometheus operator image | `quay.io/coreos/prometheus-operator` |
+| `prometheusOperator.image.repository` | Repository for prometheus operator image | `rancher/coreos-prometheus-operator` |
 | `prometheusOperator.image.tag` | Tag for prometheus operator image | `v0.38.1` |
 | `prometheusOperator.kubeletService.enabled` | If true, the operator will create and maintain a service for scraping kubelets | `true` |
 | `prometheusOperator.kubeletService.namespace` | Namespace to deploy kubelet service | `kube-system` |
@@ -222,7 +251,7 @@
 | `prometheusOperator.podAnnotations` | Annotations to add to the operator pod | `{}` |
 | `prometheusOperator.podLabels` | Labels to add to the operator pod | `{}` |
 | `prometheusOperator.priorityClassName` | Name of Priority Class to assign pods | `nil` |
-| `prometheusOperator.prometheusConfigReloaderImage.repository` | Repository for config-reloader image | `quay.io/coreos/prometheus-config-reloader` |
+| `prometheusOperator.prometheusConfigReloaderImage.repository` | Repository for config-reloader image | `rancher/coreos-prometheus-config-reloader` |
 | `prometheusOperator.prometheusConfigReloaderImage.tag` | Tag for config-reloader image | `v0.38.1` |
 | `prometheusOperator.resources` | Resource limits for prometheus operator | `{}` |
 | `prometheusOperator.securityContext` | SecurityContext for prometheus operator | `{"fsGroup": 65534, "runAsGroup": 65534, "runAsNonRoot": true, "runAsUser": 65534}` |
@@ -241,8 +270,8 @@
 | `prometheusOperator.serviceMonitor.metricRelabelings` | The `metric_relabel_configs` for scraping the operator instance. | `` |
 | `prometheusOperator.serviceMonitor.relabelings` | The `relabel_configs` for scraping the operator instance. | `` |
 | `prometheusOperator.serviceMonitor.selfMonitor` | Enable monitoring of prometheus operator | `true` |
-| `prometheusOperator.tlsProxy.enabled` | Enable a TLS proxy container. Only the `squareup/ghostunnel` command line arguments are currently supported and the secret where the cert is loaded from is expected to be provided by the admission webhook | `true` |
-| `prometheusOperator.tlsProxy.image.repository` | Repository for the TLS proxy container | `squareup/ghostunnel` |
+| `prometheusOperator.tlsProxy.enabled` | Enable a TLS proxy container. Only the `rancher/squareup-ghostunnel` command line arguments are currently supported and the secret where the cert is loaded from is expected to be provided by the admission webhook | `true` |
+| `prometheusOperator.tlsProxy.image.repository` | Repository for the TLS proxy container | `rancher/squareup-ghostunnel` |
 | `prometheusOperator.tlsProxy.image.tag` | Repository for the TLS proxy container | `v1.5.2` |
 | `prometheusOperator.tlsProxy.image.pullPolicy` | Image pull policy for the TLS proxy container | `IfNotPresent` |
 | `prometheusOperator.tlsProxy.resources` | Resource requests and limits for the TLS proxy container | `{}` |
@@ -291,7 +320,8 @@
 | `prometheus.prometheusSpec.evaluationInterval` | Interval between consecutive evaluations. | `""` |
 | `prometheus.prometheusSpec.externalLabels` | The labels to add to any time series or alerts when communicating with external systems (federation, remote storage, Alertmanager). | `{}` |
 | `prometheus.prometheusSpec.externalUrl` | The external URL the Prometheus instances will be available under. This is necessary to generate correct URLs. This is necessary if Prometheus is not served from root of a DNS name. | `""` |
-| `prometheus.prometheusSpec.image.repository` | Base image to use for a Prometheus deployment. | `quay.io/prometheus/prometheus` |
+| `prometheus.prometheusSpec.ignoreNamespaceSelectors` | Ignore NamespaceSelector settings from the PodMonitor and ServiceMonitor configs. If true, PodMonitors and ServiceMonitors can only discover Pods and Services within the namespace they are deployed into | `false` |
+| `prometheus.prometheusSpec.image.repository` | Base image to use for a Prometheus deployment. | `rancher/prom-prometheus` |
 | `prometheus.prometheusSpec.image.tag` | Tag of Prometheus container image to be deployed. | `v2.18.1` |
 | `prometheus.prometheusSpec.listenLocal` | ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP. | `false` |
 | `prometheus.prometheusSpec.logFormat` | Log format for Prometheus to be configured with. | `logfmt` |
@@ -374,7 +404,7 @@
 | `alertmanager.alertmanagerSpec.configSecret` | ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config. | `""` |
 | `alertmanager.alertmanagerSpec.containers` | Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod. | `[]` |
 | `alertmanager.alertmanagerSpec.externalUrl` | The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. | `""` |
-| `alertmanager.alertmanagerSpec.image.repository` | Base image that is used to deploy pods, without tag. | `quay.io/prometheus/alertmanager` |
+| `alertmanager.alertmanagerSpec.image.repository` | Base image that is used to deploy pods, without tag. | `rancher/prom-alertmanager` |
 | `alertmanager.alertmanagerSpec.image.tag` | Tag of Alertmanager container image to be deployed. | `v0.20.0` |
 | `alertmanager.alertmanagerSpec.listenLocal` | ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP. Note this is only for the Alertmanager UI, not the gossip communication. | `false` |
 | `alertmanager.alertmanagerSpec.logFormat` | Log format for Alertmanager to be configured with. | `logfmt` |
@@ -415,6 +445,10 @@
 | `alertmanager.podDisruptionBudget.enabled` | If true, create a pod disruption budget for Alertmanager pods. The created resource cannot be modified once created - it must be deleted to perform a change | `false` |
 | `alertmanager.podDisruptionBudget.maxUnavailable` | Maximum number / percentage of pods that may be made unavailable | `""` |
 | `alertmanager.podDisruptionBudget.minAvailable` | Minimum number / percentage of pods that should remain scheduled | `1` |
+| `alertmanager.secret.cleanupOnUninstall` | Whether or not to trigger a job to clean up the alertmanager config secret to be deleted on a `helm uninstall`. By default, this is disabled to prevent the loss of alerting configuration on an uninstall. | `false` |
+| `alertmanager.secret.image.pullPolicy` | Image pull policy for job(s) related to alertmanager config secret's lifecycle | `IfNotPresent` |
+| `alertmanager.secret.image.repository` | Repository to use for job(s) related to alertmanager config secret's lifecycle | `rancher/hyperkube` |
+| `alertmanager.secret.image.tag` | Tag to use for job(s) related to alertmanager config secret's lifecycle | `v1.18.6-rancher1` |
 | `alertmanager.secret.annotations` | Alertmanager Secret annotations | `{}` |
 | `alertmanager.service.annotations` | Alertmanager Service annotations | `{}` |
 | `alertmanager.service.clusterIP` | Alertmanager service clusterIP IP | `""` |
@@ -465,17 +499,23 @@
 | `grafana.namespaceOverride` | Override the deployment namespace of grafana | `""` (`Release.Namespace`) |
 | `grafana.rbac.pspUseAppArmor` | Enforce AppArmor in created PodSecurityPolicy (requires rbac.pspEnabled) | `true` |
 | `grafana.service.portName` | Allow to customize Grafana service portname. Will be used by servicemonitor as well | `service` |
+| `grafana.service.port` | Kubernetes port where Grafana is exposed | `80` |
+| `grafana.service.targetPort` | Internal service port for Grafana | `3000` |
+| `grafana.service.nodePort` | Port to expose on each node running Grafana | `30950` |
+| `grafana.service.type` | Type of Kubernetes Service used for Grafana | `ClusterIP` |
 | `grafana.serviceMonitor.metricRelabelings` | The `metric_relabel_configs` for scraping the grafana instance. | `` |
 | `grafana.serviceMonitor.relabelings` | The `relabel_configs` for scraping the grafana instance. | `` |
 | `grafana.serviceMonitor.selfMonitor` | Create a `serviceMonitor` to automatically monitor the grafana instance | `true` |
 | `grafana.sidecar.dashboards.enabled` | Enable the Grafana sidecar to automatically load dashboards with a label `{{ grafana.sidecar.dashboards.label }}=1` | `true` |
 | `grafana.sidecar.dashboards.annotations` | Create annotations on dashboard configmaps | `{}` |
 | `grafana.sidecar.dashboards.label` | If the sidecar is enabled, configmaps with this label will be loaded into Grafana as dashboards | `grafana_dashboard` |
+| `grafana.sidecar.dashboards.searchNamespace` | If specified, the sidecar will search for dashboard config-maps inside this namespace. Otherwise the namespace in which the sidecar is running will be used. It's also possible to specify ALL to search in all namespaces | `grafana-dashboards` |
 | `grafana.sidecar.datasources.annotations` | Create annotations on datasource configmaps | `{}` |
 | `grafana.sidecar.datasources.createPrometheusReplicasDatasources` | Create datasource for each Pod of Prometheus StatefulSet i.e. `Prometheus-0`, `Prometheus-1` | `false` |
 | `grafana.sidecar.datasources.defaultDatasourceEnabled` | Enable Grafana `Prometheus` default datasource | `true` |
 | `grafana.sidecar.datasources.enabled` | Enable the Grafana sidecar to automatically load datasources with a label `{{ grafana.sidecar.datasources.label }}=1` | `true` |
 | `grafana.sidecar.datasources.label` | If the sidecar is enabled, configmaps with this label will be loaded into Grafana as datasources configurations | `grafana_datasource` |
+| `grafana.sidecar.datasources.searchNamespace` | If specified, the sidecar will search for datasources config-maps inside this namespace. Otherwise the namespace in which the sidecar is running will be used. It's also possible to specify ALL to search in all namespaces | `grafana-datasources` |
 
 ### Exporters
 | Parameter | Description | Default |
@@ -649,7 +689,7 @@
 The Grafana chart is more feature-rich than this chart - it contains a sidecar that is able to load data sources and dashboards from configmaps deployed into the same cluster. For more information check out the [documentation for the chart](https://github.com/helm/charts/tree/master/stable/grafana)
 
 ### Coreos CRDs
-The CRDs are provisioned using crd-install hooks, rather than relying on a separate chart installation. If you already have these CRDs provisioned and don't want to remove them, you can disable the CRD creation by these hooks by passing `prometheusOperator.createCustomResource=false` (not required if using Helm v3).
+The CRDs are provisioned using a separate chart installation within the Helm chart `rancher-monitoring-crd` that is packaged alongside this chart.
 
 ### Kubelet Service
 Because the kubelet service has a new name in the chart, make sure to clean up the old kubelet service in the `kube-system` namespace to prevent counting container metrics twice.
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/grafana/README.md packages/rancher-monitoring/charts/charts/grafana/README.md
--- packages/rancher-monitoring/charts-original/charts/grafana/README.md
+++ packages/rancher-monitoring/charts/charts/grafana/README.md
@@ -51,7 +51,7 @@
 | `readinessProbe`                          | Readiness Probe settings                      | `{ "httpGet": { "path": "/api/health", "port": 3000 } }`|
 | `securityContext`                         | Deployment securityContext                    | `{"runAsUser": 472, "runAsGroup": 472, "fsGroup": 472}`  |
 | `priorityClassName`                       | Name of Priority Class to assign pods         | `nil`                                                   |
-| `image.repository`                        | Image repository                              | `grafana/grafana`                                       |
+| `image.repository`                        | Image repository                              | `rancher/grafana-grafana`                                       |
 | `image.tag`                               | Image tag (`Must be >= 5.0.0`)                | `7.0.3`                                                 |
 | `image.pullPolicy`                        | Image pull policy                             | `IfNotPresent`                                          |
 | `image.pullSecrets`                       | Image pull secrets                            | `{}`                                                    |
@@ -92,7 +92,7 @@
 | `persistence.finalizers`                  | PersistentVolumeClaim finalizers              | `[ "kubernetes.io/pvc-protection" ]`                    |
 | `persistence.subPath`                     | Mount a sub dir of the persistent volume      | `nil`                                                   |
 | `initChownData.enabled`                   | If false, don't reset data ownership at startup | true                                                  |
-| `initChownData.image.repository`          | init-chown-data container image repository    | `busybox`                                               |
+| `initChownData.image.repository`          | init-chown-data container image repository    | `rancher/busybox`                                               |
 | `initChownData.image.tag`                 | init-chown-data container image tag           | `latest`                                                |
 | `initChownData.image.pullPolicy`          | init-chown-data container image pull policy   | `IfNotPresent`                                          |
 | `initChownData.resources`                 | init-chown-data pod resource requests & limits | `{}`                                                   |
@@ -120,7 +120,7 @@
 | `podAnnotations`                          | Pod annotations                               | `{}`                                                    |
 | `podLabels`                               | Pod labels                                    | `{}`                                                    |
 | `podPortName`                             | Name of the grafana port on the pod           | `grafana`                                               |
-| `sidecar.image.repository`                | Sidecar image repository                      | `kiwigrid/k8s-sidecar`                                  |
+| `sidecar.image.repository`                | Sidecar image repository                      | `rancher/kiwigrid-k8s-sidecar`                                  |
 | `sidecar.image.tag`                       | Sidecar image tag                             | `0.1.151`                                               |
 | `sidecar.imagePullPolicy`                 | Sidecar image pull policy                     | `IfNotPresent`                                          |
 | `sidecar.resources`                       | Sidecar resources                             | `{}`                                                    |
@@ -159,13 +159,13 @@
 | `rbac.extraClusterRoleRules`              | Additional rules to add to the ClusterRole    | []                                                      |
 | `command`                     | Define command to be executed by grafana container at startup  | `nil`                                              |
 | `testFramework.enabled`                   | Whether to create test-related resources      | `true`                                                  |
-| `testFramework.image`                     | `test-framework` image repository.            | `bats/bats`                                        |
+| `testFramework.image`                     | `test-framework` image repository.            | `rancher/bats-bats`                                        |
 | `testFramework.tag`                       | `test-framework` image tag.                   | `v1.1.0`                                                 |
 | `testFramework.imagePullPolicy`           | `test-framework` image pull policy.           | `IfNotPresent`                                             |
 | `testFramework.securityContext`           | `test-framework` securityContext              | `{}`                                                    |
 | `downloadDashboards.env`                  | Environment variables to be passed to the `download-dashboards` container | `{}`                        |
 | `downloadDashboards.resources`            | Resources of `download-dashboards` container  | `{}`                                                    |
-| `downloadDashboardsImage.repository`      | Curl docker image repo                        | `curlimages/curl`                                       |
+| `downloadDashboardsImage.repository`      | Curl docker image repo                        | `rancher/curlimages-curl`                                       |
 | `downloadDashboardsImage.tag`             | Curl docker image tag                         | `7.68.0`                                                |
 | `downloadDashboardsImage.pullPolicy`      | Curl docker image pull policy                 | `IfNotPresent`                                          |
 | `namespaceOverride`                       | Override the deployment namespace             | `""` (`Release.Namespace`)                              |
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/grafana/templates/_helpers.tpl packages/rancher-monitoring/charts/charts/grafana/templates/_helpers.tpl
--- packages/rancher-monitoring/charts-original/charts/grafana/templates/_helpers.tpl
+++ packages/rancher-monitoring/charts/charts/grafana/templates/_helpers.tpl
@@ -80,3 +80,15 @@
 app.kubernetes.io/name: {{ include "grafana.name" . }}
 app.kubernetes.io/instance: {{ .Release.Name }}
 {{- end -}}
+
+{{- define "grafana_proxy_url" -}}
+{{- if .Values.global.cattle -}}
+{{- $url := required "Rancher URL required to construct proxy URL" .Values.global.cattle.url -}}
+{{- $clusterId := required "Rancher Cluster ID required to construct proxy URL" .Values.global.cattle.clusterId -}}
+{{- $proxyURLFmt := "%s/k8s/clusters/%s/api/v1/namespaces/%s/services/http:%s:%.0f/proxy" -}}
+{{- printf $proxyURLFmt $url $clusterId (include "grafana.namespace" .) (include "grafana.fullname" .) .Values.service.port -}}
+{{- else -}}
+{{- $proxyURLFmt := "http://%s-grafana.%s:%.0f" -}}
+{{- printf $proxyURLFmt (include "grafana.fullname" .) (include "grafana.namespace" .) .Values.grafana.service.port -}}
+{{- end -}}
+{{- end -}}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/grafana/templates/_pod.tpl packages/rancher-monitoring/charts/charts/grafana/templates/_pod.tpl
--- packages/rancher-monitoring/charts-original/charts/grafana/templates/_pod.tpl
+++ packages/rancher-monitoring/charts/charts/grafana/templates/_pod.tpl
@@ -15,7 +15,7 @@
 {{- end }}
 {{- if ( and .Values.persistence.enabled .Values.initChownData.enabled ) }}
   - name: init-chown-data
-    image: "{{ .Values.initChownData.image.repository }}:{{ .Values.initChownData.image.tag }}"
+    image: "{{ template "system_default_registry" . }}{{ .Values.initChownData.image.repository }}:{{ .Values.initChownData.image.tag }}"
     imagePullPolicy: {{ .Values.initChownData.image.pullPolicy }}
     securityContext:
       runAsUser: 0
@@ -31,7 +31,7 @@
 {{- end }}
 {{- if .Values.dashboards }}
   - name: download-dashboards
-    image: "{{ .Values.downloadDashboardsImage.repository }}:{{ .Values.downloadDashboardsImage.tag }}"
+    image: "{{ template "system_default_registry" . }}{{ .Values.downloadDashboardsImage.repository }}:{{ .Values.downloadDashboardsImage.tag }}"
     imagePullPolicy: {{ .Values.downloadDashboardsImage.pullPolicy }}
     command: ["/bin/sh"]
     args: [ "-c", "mkdir -p /var/lib/grafana/dashboards/default && /bin/sh /etc/grafana/download_dashboards.sh" ]
@@ -59,7 +59,7 @@
 {{- end }}
 {{- if .Values.sidecar.datasources.enabled }}
   - name: {{ template "grafana.name" . }}-sc-datasources
-    image: "{{ .Values.sidecar.image.repository }}:{{ .Values.sidecar.image.tag }}"
+    image: "{{ template "system_default_registry" . }}{{ .Values.sidecar.image.repository }}:{{ .Values.sidecar.image.tag }}"
     imagePullPolicy: {{ .Values.sidecar.imagePullPolicy }}
     env:
       - name: METHOD
@@ -96,7 +96,7 @@
 containers:
 {{- if .Values.sidecar.dashboards.enabled }}
   - name: {{ template "grafana.name" . }}-sc-dashboard
-    image: "{{ .Values.sidecar.image.repository }}:{{ .Values.sidecar.image.tag }}"
+    image: "{{ template "system_default_registry" . }}{{ .Values.sidecar.image.repository }}:{{ .Values.sidecar.image.tag }}"
     imagePullPolicy: {{ .Values.sidecar.imagePullPolicy }}
     env:
       - name: METHOD
@@ -122,7 +122,7 @@
         mountPath: {{ .Values.sidecar.dashboards.folder | quote }}
 {{- end}}
   - name: {{ .Chart.Name }}
-    image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
+    image: "{{ template "system_default_registry" . }}{{ .Values.image.repository }}:{{ .Values.image.tag }}"
     imagePullPolicy: {{ .Values.image.pullPolicy }}
   {{- if .Values.command }}
     command:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/grafana/values.yaml packages/rancher-monitoring/charts/charts/grafana/values.yaml
--- packages/rancher-monitoring/charts-original/charts/grafana/values.yaml
+++ packages/rancher-monitoring/charts/charts/grafana/values.yaml
@@ -49,8 +49,8 @@
 # schedulerName: "default-scheduler"
 
 image:
-  repository: grafana/grafana
-  tag: 7.0.5
+  repository: rancher/grafana-grafana
+  tag: 7.0.6
   pullPolicy: IfNotPresent
 
   ## Optionally specify an array of imagePullSecrets.
@@ -62,7 +62,7 @@
 
 testFramework:
   enabled: true
-  image: "bats/bats"
+  image: "rancher/bats-bats"
   tag: "v1.1.0"
   imagePullPolicy: IfNotPresent
   securityContext: {}
@@ -90,7 +90,7 @@
 # priorityClassName:
 
 downloadDashboardsImage:
-  repository: curlimages/curl
+  repository: rancher/curlimages-curl
   tag: 7.70.0
   pullPolicy: IfNotPresent
 
@@ -224,7 +224,7 @@
   ## initChownData container image
   ##
   image:
-    repository: busybox
+    repository: rancher/busybox
     tag: "1.31.1"
     pullPolicy: IfNotPresent
 
@@ -405,6 +405,9 @@
     mode: console
   grafana_net:
     url: https://grafana.net
+  server:
+    root_url: /api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-grafana:80/proxy
+    serve_from_sub_path: true
 ## grafana Authentication can be enabled with the following values on grafana.ini
  # server:
       # The full public facing url you use in browser, used for redirects and emails
@@ -465,7 +468,7 @@
 ## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards
 sidecar:
   image:
-    repository: kiwigrid/k8s-sidecar
+    repository: rancher/kiwigrid-k8s-sidecar
     tag: 0.1.151
   imagePullPolicy: IfNotPresent
   resources: {}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/kube-state-metrics/README.md packages/rancher-monitoring/charts/charts/kube-state-metrics/README.md
--- packages/rancher-monitoring/charts-original/charts/kube-state-metrics/README.md
+++ packages/rancher-monitoring/charts/charts/kube-state-metrics/README.md
@@ -14,7 +14,7 @@
 
 | Parameter                                    | Description                                                                           | Default                                    |
 |:---------------------------------------------|:--------------------------------------------------------------------------------------|:-------------------------------------------|
-| `image.repository`                           | The image repository to pull from                                                     | `quay.io/coreos/kube-state-metrics`        |
+| `image.repository`                           | The image repository to pull from                                                     | `rancher/coreos-kube-state-metrics`        |
 | `image.tag`                                  | The image tag to pull from                                                            | `v1.9.7`                                   |
 | `image.pullPolicy`                           | Image pull policy                                                                     | `IfNotPresent`                             |
 | `imagePullSecrets`                           | List of container registry secrets                                                    | `[]`                                       |
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/kube-state-metrics/templates/deployment.yaml packages/rancher-monitoring/charts/charts/kube-state-metrics/templates/deployment.yaml
--- packages/rancher-monitoring/charts-original/charts/kube-state-metrics/templates/deployment.yaml
+++ packages/rancher-monitoring/charts/charts/kube-state-metrics/templates/deployment.yaml
@@ -154,7 +154,7 @@
         - --pod-namespace=$(POD_NAMESPACE)
 {{ end }}
         imagePullPolicy: {{ .Values.image.pullPolicy }}
-        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
+        image: "{{ template "system_default_registry" . }}{{ .Values.image.repository }}:{{ .Values.image.tag }}"
         ports:
         - containerPort: 8080
         livenessProbe:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/kube-state-metrics/values.yaml packages/rancher-monitoring/charts/charts/kube-state-metrics/values.yaml
--- packages/rancher-monitoring/charts-original/charts/kube-state-metrics/values.yaml
+++ packages/rancher-monitoring/charts/charts/kube-state-metrics/values.yaml
@@ -1,7 +1,7 @@
 # Default values for kube-state-metrics.
 prometheusScrape: true
 image:
-  repository: quay.io/coreos/kube-state-metrics
+  repository: rancher/coreos-kube-state-metrics
   tag: v1.9.7
   pullPolicy: IfNotPresent
 
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/prometheus-adapter/README.md packages/rancher-monitoring/charts/charts/prometheus-adapter/README.md
--- packages/rancher-monitoring/charts-original/charts/prometheus-adapter/README.md
+++ packages/rancher-monitoring/charts/charts/prometheus-adapter/README.md
@@ -111,7 +111,7 @@
 | Parameter                       | Description                                                                     | Default                                     |
 | ------------------------------- | ------------------------------------------------------------------------------- | --------------------------------------------|
 | `affinity`                      | Node affinity                                                                   | `{}`                                        |
-| `image.repository`              | Image repository                                                                | `directxman12/k8s-prometheus-adapter-amd64` |
+| `image.repository`              | Image repository                                                                | `rancher/directxman12-k8s-prometheus-adapter-amd64` |
 | `image.tag`                     | Image tag                                                                       | `v0.6.0`                                    |
 | `image.pullPolicy`              | Image pull policy                                                               | `IfNotPresent`                              |
 | `image.pullSecrets`             | Image pull secrets                                                              | `{}`                                        |
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/prometheus-adapter/templates/custom-metrics-apiserver-deployment.yaml packages/rancher-monitoring/charts/charts/prometheus-adapter/templates/custom-metrics-apiserver-deployment.yaml
--- packages/rancher-monitoring/charts-original/charts/prometheus-adapter/templates/custom-metrics-apiserver-deployment.yaml
+++ packages/rancher-monitoring/charts/charts/prometheus-adapter/templates/custom-metrics-apiserver-deployment.yaml
@@ -36,7 +36,7 @@
       {{- end }}
       containers:
       - name: {{ .Chart.Name }}
-        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
+        image: "{{ template "system_default_registry" . }}{{ .Values.image.repository }}:{{ .Values.image.tag }}"
         imagePullPolicy: {{ .Values.image.pullPolicy }}
         args:
         - /adapter
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/prometheus-adapter/values.yaml packages/rancher-monitoring/charts/charts/prometheus-adapter/values.yaml
--- packages/rancher-monitoring/charts-original/charts/prometheus-adapter/values.yaml
+++ packages/rancher-monitoring/charts/charts/prometheus-adapter/values.yaml
@@ -2,7 +2,7 @@
 affinity: {}
 
 image:
-  repository: directxman12/k8s-prometheus-adapter-amd64
+  repository: rancher/directxman12-k8s-prometheus-adapter-amd64
   tag: v0.6.0
   pullPolicy: IfNotPresent
 
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/prometheus-node-exporter/README.md packages/rancher-monitoring/charts/charts/prometheus-node-exporter/README.md
--- packages/rancher-monitoring/charts-original/charts/prometheus-node-exporter/README.md
+++ packages/rancher-monitoring/charts/charts/prometheus-node-exporter/README.md
@@ -38,7 +38,7 @@
 
 |             Parameter                 |                                                          Description                                                          |                 Default                          |
 | ------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
-| `image.repository`                    | Image repository                                                                                                              | `quay.io/prometheus/node-exporter`               |
+| `image.repository`                    | Image repository                                                                                                              | `rancher/prom-node-exporter`               |
 | `image.tag`                           | Image tag                                                                                                                     | `v1.0.0`                                         |
 | `image.pullPolicy`                    | Image pull policy                                                                                                             | `IfNotPresent`                                   |
 | `extraArgs`                           | Additional container arguments                                                                                                | `[]`                                             |
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/prometheus-node-exporter/templates/daemonset.yaml packages/rancher-monitoring/charts/charts/prometheus-node-exporter/templates/daemonset.yaml
--- packages/rancher-monitoring/charts-original/charts/prometheus-node-exporter/templates/daemonset.yaml
+++ packages/rancher-monitoring/charts/charts/prometheus-node-exporter/templates/daemonset.yaml
@@ -33,7 +33,7 @@
 {{- end }}
       containers:
         - name: node-exporter
-          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
+          image: "{{ template "system_default_registry" . }}{{ .Values.image.repository }}:{{ .Values.image.tag }}"
           imagePullPolicy: {{ .Values.image.pullPolicy }}
           args:
             - --path.procfs=/host/proc
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/charts/prometheus-node-exporter/values.yaml packages/rancher-monitoring/charts/charts/prometheus-node-exporter/values.yaml
--- packages/rancher-monitoring/charts-original/charts/prometheus-node-exporter/values.yaml
+++ packages/rancher-monitoring/charts/charts/prometheus-node-exporter/values.yaml
@@ -2,7 +2,7 @@
 # This is a YAML-formatted file.
 # Declare variables to be passed into your templates.
 image:
-  repository: quay.io/prometheus/node-exporter
+  repository: rancher/prom-node-exporter
   tag: v1.0.0
   pullPolicy: IfNotPresent
 
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/crds/crd-alertmanager.yaml packages/rancher-monitoring/charts/crds/crd-alertmanager.yaml
--- packages/rancher-monitoring/charts-original/crds/crd-alertmanager.yaml
+++ packages/rancher-monitoring/charts/crds/crd-alertmanager.yaml
@@ -4,7 +4,6 @@
 metadata:
   annotations:
     controller-gen.kubebuilder.io/version: v0.2.4
-    helm.sh/hook: crd-install
   creationTimestamp: null
   name: alertmanagers.monitoring.coreos.com
 spec:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/crds/crd-podmonitor.yaml packages/rancher-monitoring/charts/crds/crd-podmonitor.yaml
--- packages/rancher-monitoring/charts-original/crds/crd-podmonitor.yaml
+++ packages/rancher-monitoring/charts/crds/crd-podmonitor.yaml
@@ -4,7 +4,6 @@
 metadata:
   annotations:
     controller-gen.kubebuilder.io/version: v0.2.4
-    helm.sh/hook: crd-install
   creationTimestamp: null
   name: podmonitors.monitoring.coreos.com
 spec:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/crds/crd-prometheus.yaml packages/rancher-monitoring/charts/crds/crd-prometheus.yaml
--- packages/rancher-monitoring/charts-original/crds/crd-prometheus.yaml
+++ packages/rancher-monitoring/charts/crds/crd-prometheus.yaml
@@ -4,7 +4,6 @@
 metadata:
   annotations:
     controller-gen.kubebuilder.io/version: v0.2.4
-    helm.sh/hook: crd-install
   creationTimestamp: null
   name: prometheuses.monitoring.coreos.com
 spec:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/crds/crd-prometheusrules.yaml packages/rancher-monitoring/charts/crds/crd-prometheusrules.yaml
--- packages/rancher-monitoring/charts-original/crds/crd-prometheusrules.yaml
+++ packages/rancher-monitoring/charts/crds/crd-prometheusrules.yaml
@@ -4,7 +4,6 @@
 metadata:
   annotations:
     controller-gen.kubebuilder.io/version: v0.2.4
-    helm.sh/hook: crd-install
   creationTimestamp: null
   name: prometheusrules.monitoring.coreos.com
 spec:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/crds/crd-servicemonitor.yaml packages/rancher-monitoring/charts/crds/crd-servicemonitor.yaml
--- packages/rancher-monitoring/charts-original/crds/crd-servicemonitor.yaml
+++ packages/rancher-monitoring/charts/crds/crd-servicemonitor.yaml
@@ -4,7 +4,6 @@
 metadata:
   annotations:
     controller-gen.kubebuilder.io/version: v0.2.4
-    helm.sh/hook: crd-install
   creationTimestamp: null
   name: servicemonitors.monitoring.coreos.com
 spec:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/crds/crd-thanosrulers.yaml packages/rancher-monitoring/charts/crds/crd-thanosrulers.yaml
--- packages/rancher-monitoring/charts-original/crds/crd-thanosrulers.yaml
+++ packages/rancher-monitoring/charts/crds/crd-thanosrulers.yaml
@@ -4,7 +4,6 @@
 metadata:
   annotations:
     controller-gen.kubebuilder.io/version: v0.2.4
-    helm.sh/hook: crd-install
   creationTimestamp: null
   name: thanosrulers.monitoring.coreos.com
 spec:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/requirements.yaml packages/rancher-monitoring/charts/requirements.yaml
--- packages/rancher-monitoring/charts-original/requirements.yaml
+++ packages/rancher-monitoring/charts/requirements.yaml
@@ -1,16 +1,111 @@
 dependencies:
 
   - name: kube-state-metrics
-    version: "2.8.*"
+    version: 2.8.14
     repository: https://kubernetes-charts.storage.googleapis.com/
     condition: kubeStateMetrics.enabled
 
   - name: prometheus-node-exporter
-    version: "1.10.*"
+    version: 1.10.0
     repository: https://kubernetes-charts.storage.googleapis.com/
     condition: nodeExporter.enabled
 
   - name: grafana
-    version: "5.3.*"
+    version: 5.3.6
     repository: https://kubernetes-charts.storage.googleapis.com/
     condition: grafana.enabled
+
+  - name: prometheus-adapter
+    version: 2.4.0
+    repository: https://kubernetes-charts.storage.googleapis.com/
+    condition: prometheus-adapter.enabled
+
+  - name: rancher-pushprox
+    alias: rkeControllerManager
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: rkeControllerManager.enabled
+
+  - name: rancher-pushprox
+    alias: rkeScheduler
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: rkeScheduler.enabled
+
+  - name: rancher-pushprox
+    alias: rkeProxy
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: rkeProxy.enabled
+
+  - name: rancher-pushprox
+    alias: rkeEtcd
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: rkeEtcd.enabled
+
+  - name: rancher-pushprox
+    alias: k3sControllerManager
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: k3sControllerManager.enabled
+
+  - name: rancher-pushprox
+    alias: k3sScheduler
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: k3sScheduler.enabled
+  
+  - name: rancher-pushprox
+    alias: k3sProxy
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: k3sProxy.enabled
+
+  - name: rancher-pushprox
+    alias: kubeAdmControllerManager
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: kubeAdmControllerManager.enabled
+
+  - name: rancher-pushprox
+    alias: kubeAdmScheduler
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: kubeAdmScheduler.enabled
+
+  - name: rancher-pushprox
+    alias: kubeAdmProxy
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: kubeAdmProxy.enabled
+
+  - name: rancher-pushprox
+    alias: kubeAdmEtcd
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: kubeAdmEtcd.enabled
+
+  - name: rancher-pushprox
+    alias: rke2ControllerManager
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: rke2ControllerManager.enabled
+
+  - name: rancher-pushprox
+    alias: rke2Scheduler
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: rke2Scheduler.enabled
+
+  - name: rancher-pushprox
+    alias: rke2Proxy
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: rke2Proxy.enabled
+
+  - name: rancher-pushprox
+    alias: rke2Etcd
+    version: 0.1.0
+    repository: file://../../rancher-pushprox/charts
+    condition: rke2Etcd.enabled
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/_helpers.tpl packages/rancher-monitoring/charts/templates/_helpers.tpl
--- packages/rancher-monitoring/charts-original/templates/_helpers.tpl
+++ packages/rancher-monitoring/charts/templates/_helpers.tpl
@@ -1,3 +1,37 @@
+# Rancher
+{{- define "system_default_registry" -}}
+{{- if .Values.global.cattle.systemDefaultRegistry -}}
+{{- printf "%s/" .Values.global.cattle.systemDefaultRegistry -}}
+{{- end -}}
+{{- end -}}
+
+# Special Exporters
+{{- define "exporter.kubeEtcd.enabled" -}}
+{{- if or .Values.kubeEtcd.enabled .Values.rkeEtcd.enabled .Values.kubeAdmEtcd.enabled .Values.rke2Etcd.enabled -}}
+"true"
+{{- end -}}
+{{- end }}
+
+{{- define "exporter.kubeControllerManager.enabled" -}}
+{{- if or .Values.kubeControllerManager.enabled .Values.rkeControllerManager.enabled .Values.k3sControllerManager.enabled .Values.kubeAdmControllerManager.enabled .Values.rke2ControllerManager.enabled -}}
+"true"
+{{- end -}}
+{{- end }}
+
+{{- define "exporter.kubeScheduler.enabled" -}}
+{{- if or .Values.kubeScheduler.enabled .Values.rkeScheduler.enabled .Values.k3sScheduler.enabled .Values.kubeAdmScheduler.enabled .Values.rke2Scheduler.enabled -}}
+"true"
+{{- end -}}
+{{- end }}
+
+{{- define "exporter.kubeProxy.enabled" -}}
+{{- if or .Values.kubeProxy.enabled .Values.rkeProxy.enabled .Values.k3sProxy.enabled .Values.kubeAdmProxy.enabled .Values.rke2Proxy.enabled -}}
+"true"
+{{- end -}}
+{{- end }}
+
+# Prometheus Operator
+
 {{/* vim: set filetype=mustache: */}}
 {{/* Expand the name of the chart. This is suffixed with -alertmanager, which means subtract 13 from longest 63 available */}}
 {{- define "prometheus-operator.name" -}}
@@ -90,4 +124,28 @@
   {{- else -}}
     {{- .Release.Namespace -}}
   {{- end -}}
+{{- end -}}
+
+{{- define "prometheus_proxy_url" -}}
+{{- if .Values.global.cattle -}}
+{{- $url := required "Rancher URL required to construct proxy URL" .Values.global.cattle.url -}}
+{{- $clusterId := required "Rancher Cluster ID required to construct proxy URL" .Values.global.cattle.clusterId -}}
+{{- $proxyURLFmt := "%s/k8s/clusters/%s/api/v1/namespaces/%s/services/http:%s-prometheus:%.0f/proxy" -}}
+{{- printf $proxyURLFmt $url $clusterId (include "prometheus-operator.namespace" .) (include "prometheus-operator.fullname" .) .Values.prometheus.service.port -}}
+{{- else -}}
+{{- $proxyURLFmt := "http://%s-prometheus.%s:%.0f" -}}
+{{- printf $proxyURLFmt (include "prometheus-operator.fullname" .) (include "prometheus-operator.namespace" .) .Values.prometheus.service.port -}}
+{{- end -}}
+{{- end -}}
+
+{{- define "alertmanager_proxy_url" -}}
+{{- if .Values.global.cattle -}}
+{{- $url := required "Rancher URL required to construct proxy URL" .Values.global.cattle.url -}}
+{{- $clusterId := required "Rancher Cluster ID required to construct proxy URL" .Values.global.cattle.clusterId -}}
+{{- $proxyURLFmt := "%s/k8s/clusters/%s/api/v1/namespaces/%s/services/http:%s-alertmanager:%.0f/proxy" -}}
+{{- printf $proxyURLFmt $url $clusterId (include "prometheus-operator.namespace" .) (include "prometheus-operator.fullname" .) .Values.alertmanager.service.port -}}
+{{- else -}}
+{{- $proxyURLFmt := "http://%s-alertmanager.%s:%.0f" -}}
+{{- printf $proxyURLFmt (include "prometheus-operator.fullname" .) (include "prometheus-operator.namespace" .) .Values.prometheus.service.port -}}
+{{- end -}}
 {{- end -}}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/alertmanager/alertmanager.yaml packages/rancher-monitoring/charts/templates/alertmanager/alertmanager.yaml
--- packages/rancher-monitoring/charts-original/templates/alertmanager/alertmanager.yaml
+++ packages/rancher-monitoring/charts/templates/alertmanager/alertmanager.yaml
@@ -9,7 +9,7 @@
 {{ include "prometheus-operator.labels" . | indent 4 }}
 spec:
 {{- if .Values.alertmanager.alertmanagerSpec.image }}
-  baseImage: {{ .Values.alertmanager.alertmanagerSpec.image.repository }}
+  baseImage: {{ template "system_default_registry" . }}{{ .Values.alertmanager.alertmanagerSpec.image.repository }}
   version: {{ .Values.alertmanager.alertmanagerSpec.image.tag }}
 {{- end }}
   replicas: {{ .Values.alertmanager.alertmanagerSpec.replicas }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/alertmanager/cleanupSecret.yaml packages/rancher-monitoring/charts/templates/alertmanager/cleanupSecret.yaml
--- packages/rancher-monitoring/charts-original/templates/alertmanager/cleanupSecret.yaml
+++ packages/rancher-monitoring/charts/templates/alertmanager/cleanupSecret.yaml
@@ -0,0 +1,82 @@
+{{- if and (.Values.alertmanager.enabled) (not .Values.alertmanager.alertmanagerSpec.useExistingSecret) (.Values.alertmanager.secret.cleanupOnUninstall) }}
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-post-delete
+  namespace: {{ template "prometheus-operator.namespace" . }}
+  labels:
+{{ include "prometheus-operator.labels" . | indent 4 }}
+    app: {{ template "prometheus-operator.name" . }}-alertmanager
+  annotations:
+    "helm.sh/hook": post-delete
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "5"
+spec:
+  template:
+    metadata:
+      name: alertmanager-{{ template "prometheus-operator.fullname" . }}-post-delete
+      labels: {{ include "prometheus-operator.labels" . | nindent 8 }}
+        app: {{ template "prometheus-operator.name" . }}-alertmanager
+    spec:
+      serviceAccountName: alertmanager-{{ template "prometheus-operator.fullname" . }}-post-delete
+      containers:
+        - name: delete-secret
+          image: {{ template "system_default_registry" . }}{{ .Values.alertmanager.secret.image.repository }}:{{ .Values.alertmanager.secret.image.tag }}
+          imagePullPolicy: {{ .Values.alertmanager.secret.image.pullPolicy }}
+          command:
+          - /bin/sh
+          - -c
+          - >
+            if kubectl get secret -n {{ template "prometheus-operator.namespace" . }} alertmanager-{{ template "prometheus-operator.fullname" . }}-alertmanager > /dev/null 2>&1; then
+              kubectl delete secret -n {{ template "prometheus-operator.namespace" . }} alertmanager-{{ template "prometheus-operator.fullname" . }}-alertmanager
+            fi;
+      restartPolicy: OnFailure
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-post-delete
+  labels:
+    app: {{ template "prometheus-operator.name" . }}-alertmanager
+  annotations:
+    "helm.sh/hook": post-delete
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "3"
+rules:
+- apiGroups:
+  - ""
+  resources:
+  - secrets
+  verbs: ['get', 'delete']
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-post-delete
+  labels:
+    app: {{ template "prometheus-operator.name" . }}-alertmanager
+  annotations:
+    "helm.sh/hook": post-delete
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "3"
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-post-delete
+subjects:
+- kind: ServiceAccount
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-post-delete
+  namespace: {{ template "prometheus-operator.namespace" . }}
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-post-delete
+  namespace: {{ template "prometheus-operator.namespace" . }}
+  labels:
+    app: {{ template "prometheus-operator.name" . }}-alertmanager
+  annotations:
+    "helm.sh/hook": post-delete
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "3"
+{{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/alertmanager/secret.yaml packages/rancher-monitoring/charts/templates/alertmanager/secret.yaml
--- packages/rancher-monitoring/charts-original/templates/alertmanager/secret.yaml
+++ packages/rancher-monitoring/charts/templates/alertmanager/secret.yaml
@@ -1,11 +1,19 @@
 {{- if and (.Values.alertmanager.enabled) (not .Values.alertmanager.alertmanagerSpec.useExistingSecret) }}
+{{- if .Release.IsInstall }}
+{{- $secretName := (printf "alertmanager-%s-alertmanager" (include "prometheus-operator.fullname" .)) }}
+{{- if (lookup "v1" "Secret" (include "prometheus-operator.namespace" .) $secretName) }}
+{{- required (printf "Cannot overwrite existing secret %s in namespace %s." $secretName (include "prometheus-operator.namespace" .)) "" }}
+{{- end }}{{- end }}
 apiVersion: v1
 kind: Secret
 metadata:
-  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-alertmanager
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
   namespace: {{ template "prometheus-operator.namespace" . }}
-{{- if .Values.alertmanager.secret.annotations }}
   annotations:
+    "helm.sh/hook": pre-install
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "4"
+{{- if .Values.alertmanager.secret.annotations }}
 {{ toYaml .Values.alertmanager.secret.annotations | indent 4 }}
 {{- end }}
   labels:
@@ -20,4 +28,93 @@
 {{- range $key, $val := .Values.alertmanager.templateFiles }}
   {{ $key }}: {{ $val | b64enc | quote }}
 {{- end }}
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+  namespace: {{ template "prometheus-operator.namespace" . }}
+  labels:
+{{ include "prometheus-operator.labels" . | indent 4 }}
+    app: {{ template "prometheus-operator.name" . }}-alertmanager
+  annotations:
+    "helm.sh/hook": pre-install
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "5"
+spec:
+  template:
+    metadata:
+      name: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+      labels: {{ include "prometheus-operator.labels" . | nindent 8 }}
+        app: {{ template "prometheus-operator.name" . }}-alertmanager
+    spec:
+      serviceAccountName: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+      containers:
+        - name: copy-pre-install-secret
+          image: {{ template "system_default_registry" . }}{{ .Values.alertmanager.secret.image.repository }}:{{ .Values.alertmanager.secret.image.tag }}
+          imagePullPolicy: {{ .Values.alertmanager.secret.image.pullPolicy }}
+          command:
+          - /bin/sh
+          - -c
+          - >
+            if kubectl get secret -n {{ template "prometheus-operator.namespace" . }} alertmanager-{{ template "prometheus-operator.fullname" . }}-alertmanager > /dev/null 2>&1; then
+              echo "Secret already exists"
+              exit 1
+            fi;
+            kubectl patch secret -n {{ template "prometheus-operator.namespace" . }} --dry-run -o yaml
+            alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+            -p '{{ printf "{\"metadata\":{\"name\": \"alertmanager-%s-alertmanager\"}}" (include "prometheus-operator.fullname" .) }}'
+            | kubectl apply -f -;
+            kubectl annotate secret -n {{ template "prometheus-operator.namespace" . }}
+            alertmanager-{{ template "prometheus-operator.fullname" . }}-alertmanager
+            helm.sh/hook- helm.sh/hook-delete-policy- helm.sh/hook-weight-;
+      restartPolicy: OnFailure
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+  labels:
+    app: {{ template "prometheus-operator.name" . }}-alertmanager
+  annotations:
+    "helm.sh/hook": pre-install
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "3"
+rules:
+- apiGroups:
+  - ""
+  resources:
+  - secrets
+  verbs: ['create', 'get', 'patch']
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+  labels:
+    app: {{ template "prometheus-operator.name" . }}-alertmanager
+  annotations:
+    "helm.sh/hook": pre-install
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "3"
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+subjects:
+- kind: ServiceAccount
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+  namespace: {{ template "prometheus-operator.namespace" . }}
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: alertmanager-{{ template "prometheus-operator.fullname" . }}-pre-install
+  namespace: {{ template "prometheus-operator.namespace" . }}
+  labels:
+    app: {{ template "prometheus-operator.name" . }}-alertmanager
+  annotations:
+    "helm.sh/hook": pre-install
+    "helm.sh/hook-delete-policy": hook-succeeded, hook-failed
+    "helm.sh/hook-weight": "3"
 {{- end }}
@@ -20,7 +20,7 @@
 {{- else if and .Values.alertmanager.ingress.enabled .Values.alertmanager.ingress.hosts }}
   externalUrl: "http://{{ index .Values.alertmanager.ingress.hosts 0 }}{{ .Values.alertmanager.alertmanagerSpec.routePrefix }}"
 {{- else }}
-  externalUrl: http://{{ template "prometheus-operator.fullname" . }}-alertmanager.{{ template "prometheus-operator.namespace" . }}:{{ .Values.alertmanager.service.port }}
+  externalUrl: {{ template "alertmanager_proxy_url" . }}
 {{- end }}
 {{- if .Values.alertmanager.alertmanagerSpec.nodeSelector }}
   nodeSelector:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/exporters/core-dns/servicemonitor.yaml packages/rancher-monitoring/charts/templates/exporters/core-dns/servicemonitor.yaml
--- packages/rancher-monitoring/charts-original/templates/exporters/core-dns/servicemonitor.yaml
+++ packages/rancher-monitoring/charts/templates/exporters/core-dns/servicemonitor.yaml
@@ -3,7 +3,7 @@
 kind: ServiceMonitor
 metadata:
   name: {{ template "prometheus-operator.fullname" . }}-coredns
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: "kube-system"
   labels:
     app: {{ template "prometheus-operator.name" . }}-coredns
 {{ include "prometheus-operator.labels" . | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/exporters/kube-api-server/servicemonitor.yaml packages/rancher-monitoring/charts/templates/exporters/kube-api-server/servicemonitor.yaml
--- packages/rancher-monitoring/charts-original/templates/exporters/kube-api-server/servicemonitor.yaml
+++ packages/rancher-monitoring/charts/templates/exporters/kube-api-server/servicemonitor.yaml
@@ -3,7 +3,7 @@
 kind: ServiceMonitor
 metadata:
   name: {{ template "prometheus-operator.fullname" . }}-apiserver
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: default
   labels:
     app: {{ template "prometheus-operator.name" . }}-apiserver
 {{ include "prometheus-operator.labels" . | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/exporters/kube-controller-manager/servicemonitor.yaml packages/rancher-monitoring/charts/templates/exporters/kube-controller-manager/servicemonitor.yaml
--- packages/rancher-monitoring/charts-original/templates/exporters/kube-controller-manager/servicemonitor.yaml
+++ packages/rancher-monitoring/charts/templates/exporters/kube-controller-manager/servicemonitor.yaml
@@ -3,7 +3,7 @@
 kind: ServiceMonitor
 metadata:
   name: {{ template "prometheus-operator.fullname" . }}-kube-controller-manager
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: "kube-system"
   labels:
     app: {{ template "prometheus-operator.name" . }}-kube-controller-manager
 {{ include "prometheus-operator.labels" . | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/exporters/kubelet/servicemonitor.yaml packages/rancher-monitoring/charts/templates/exporters/kubelet/servicemonitor.yaml
--- packages/rancher-monitoring/charts-original/templates/exporters/kubelet/servicemonitor.yaml
+++ packages/rancher-monitoring/charts/templates/exporters/kubelet/servicemonitor.yaml
@@ -3,7 +3,7 @@
 kind: ServiceMonitor
 metadata:
   name: {{ template "prometheus-operator.fullname" . }}-kubelet
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.kubelet.namespace }}
   labels:
     app: {{ template "prometheus-operator.name" . }}-kubelet
 {{- include "prometheus-operator.labels" . | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/configmap-dashboards.yaml packages/rancher-monitoring/charts/templates/grafana/configmap-dashboards.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/configmap-dashboards.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/configmap-dashboards.yaml
@@ -10,7 +10,7 @@
   kind: ConfigMap
   metadata:
     name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) $dashboardName | trunc 63 | trimSuffix "-" }}
-    namespace: {{ template "prometheus-operator.namespace" . }}
+    namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
     labels:
       {{- if $.Values.grafana.sidecar.dashboards.label }}
       {{ $.Values.grafana.sidecar.dashboards.label }}: "1"
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/configmaps-datasources.yaml packages/rancher-monitoring/charts/templates/grafana/configmaps-datasources.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/configmaps-datasources.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/configmaps-datasources.yaml
@@ -3,7 +3,7 @@
 kind: ConfigMap
 metadata:
   name: {{ template "prometheus-operator.fullname" . }}-grafana-datasource
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.datasources.searchNamespace }}
 {{- if .Values.grafana.sidecar.datasources.annotations }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.datasources.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/etcd.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/etcd.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/etcd.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/etcd.yaml
@@ -4,11 +4,12 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.10.0-0" $kubeTargetVersion) (semverCompare "<1.14.0-0" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled .Values.kubeEtcd.enabled }}
+{{- if and (semverCompare ">=1.10.0-0" $kubeTargetVersion) (semverCompare "<1.14.0-0" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled }}
+{{- if (include "exporter.kubeEtcd.enabled" .)}}
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "etcd" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
@@ -1113,4 +1114,5 @@
         "uid": "c2f4e12cdf69feb95caa41a5a1b423d9",
         "version": 215
     }
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-cluster-rsrc-use.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-cluster-rsrc-use.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-cluster-rsrc-use.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-cluster-rsrc-use.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-cluster-rsrc-use" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-node-rsrc-use.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-node-rsrc-use.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-node-rsrc-use.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-node-rsrc-use.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-node-rsrc-use" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-cluster.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-cluster.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-cluster.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-cluster.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-cluster" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-namespace.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-namespace.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-namespace.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-namespace.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-namespace" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-pod.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-pod.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-pod.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-pod.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-pod" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-workload.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-workload.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-workload.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-workload.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-workload" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-workloads-namespace.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-workloads-namespace.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/k8s-resources-workloads-namespace.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/k8s-resources-workloads-namespace.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-workloads-namespace" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/nodes.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/nodes.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/nodes.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/nodes.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "nodes" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/persistentvolumesusage.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/persistentvolumesusage.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/persistentvolumesusage.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/persistentvolumesusage.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "persistentvolumesusage" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/pods.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/pods.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/pods.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/pods.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "pods" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards/statefulset.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards/statefulset.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards/statefulset.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards/statefulset.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "statefulset" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/apiserver.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/apiserver.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/apiserver.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/apiserver.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "apiserver" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/cluster-total.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/cluster-total.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/cluster-total.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/cluster-total.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "cluster-total" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/controller-manager.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/controller-manager.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/controller-manager.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/controller-manager.yaml
@@ -4,11 +4,12 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled .Values.kubeControllerManager.enabled }}
+{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled }}
+{{- if (include "exporter.kubeControllerManager.enabled" .)}}
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "controller-manager" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
@@ -1139,4 +1140,5 @@
         "uid": "72e0e05bef5099e5f049b05fdc429ed4",
         "version": 0
     }
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/etcd.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/etcd.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/etcd.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/etcd.yaml
@@ -4,11 +4,12 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled .Values.kubeEtcd.enabled }}
+{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled }}
+{{- if (include "exporter.kubeEtcd.enabled" .)}}
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "etcd" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
@@ -1113,4 +1114,5 @@
         "uid": "c2f4e12cdf69feb95caa41a5a1b423d9",
         "version": 215
     }
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-coredns.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-coredns.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-coredns.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-coredns.yaml
@@ -4,7 +4,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-coredns" | trunc 63 | trimSuffix "-" }}
   labels:
     {{- if $.Values.grafana.sidecar.dashboards.label }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-cluster.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-cluster.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-cluster.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-cluster.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-cluster" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-namespace.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-namespace.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-namespace.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-namespace.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-namespace" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-node.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-node.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-node.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-node.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-node" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-pod.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-pod.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-pod.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-pod.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-pod" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-workload.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-workload.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-workload.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-workload.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-workload" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-workloads-namespace.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-workloads-namespace.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/k8s-resources-workloads-namespace.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/k8s-resources-workloads-namespace.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "k8s-resources-workloads-namespace" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/kubelet.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/kubelet.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/kubelet.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/kubelet.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "kubelet" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/namespace-by-pod.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/namespace-by-pod.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/namespace-by-pod.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/namespace-by-pod.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "namespace-by-pod" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/namespace-by-workload.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/namespace-by-workload.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/namespace-by-workload.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/namespace-by-workload.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "namespace-by-workload" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/node-cluster-rsrc-use.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/node-cluster-rsrc-use.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/node-cluster-rsrc-use.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/node-cluster-rsrc-use.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "node-cluster-rsrc-use" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/node-rsrc-use.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/node-rsrc-use.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/node-rsrc-use.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/node-rsrc-use.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "node-rsrc-use" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/nodes.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/nodes.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/nodes.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/nodes.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "nodes" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/persistentvolumesusage.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/persistentvolumesusage.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/persistentvolumesusage.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/persistentvolumesusage.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "persistentvolumesusage" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/pod-total.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/pod-total.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/pod-total.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/pod-total.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "pod-total" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/prometheus-remote-write.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/prometheus-remote-write.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/prometheus-remote-write.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/prometheus-remote-write.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "prometheus-remote-write" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/prometheus.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/prometheus.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/prometheus.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/prometheus.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "prometheus" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/proxy.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/proxy.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/proxy.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/proxy.yaml
@@ -4,11 +4,12 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled .Values.kubeProxy.enabled }}
+{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled }}
+{{- if (include "exporter.kubeProxy.enabled" .)}}
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "proxy" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
@@ -1218,4 +1219,4 @@
         "uid": "632e265de029684c40b21cb76bca4f94",
         "version": 0
     }
-{{- end }}
\ No newline at end of file
+{{- end }}{{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/scheduler.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/scheduler.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/scheduler.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/scheduler.yaml
@@ -4,11 +4,12 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled .Values.kubeScheduler.enabled }}
+{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.grafana.enabled .Values.grafana.defaultDashboardsEnabled }}
+{{- if (include "exporter.kubeScheduler.enabled" .)}}
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "scheduler" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
@@ -1063,4 +1064,5 @@
         "uid": "2e6b6a3b4bddf1427b3a55aa1311c656",
         "version": 0
     }
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/statefulset.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/statefulset.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/statefulset.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/statefulset.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "statefulset" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/workload-total.yaml packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/workload-total.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/dashboards-1.14/workload-total.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/dashboards-1.14/workload-total.yaml
@@ -8,7 +8,7 @@
 apiVersion: v1
 kind: ConfigMap
 metadata:
-  namespace: {{ template "prometheus-operator.namespace" . }}
+  namespace: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
   name: {{ printf "%s-%s" (include "prometheus-operator.fullname" $) "workload-total" | trunc 63 | trimSuffix "-" }}
   annotations:
 {{ toYaml .Values.grafana.sidecar.dashboards.annotations | indent 4 }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/grafana/namespaces.yaml packages/rancher-monitoring/charts/templates/grafana/namespaces.yaml
--- packages/rancher-monitoring/charts-original/templates/grafana/namespaces.yaml
+++ packages/rancher-monitoring/charts/templates/grafana/namespaces.yaml
@@ -0,0 +1,19 @@
+{{- if and .Values.grafana.enabled }}
+{{- if or .Values.grafana.sidecar.dashboards.enabled .Values.grafana.defaultDashboardsEnabled }}
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
+  labels:
+    name: {{ .Values.grafana.sidecar.dashboards.searchNamespace }}
+{{- end }}
+---
+{{- if or .Values.grafana.sidecar.dashboards.enabled .Values.grafana.defaultDashboardsEnabled }}
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: {{ .Values.grafana.sidecar.datasources.searchNamespace }}
+  labels:
+    name: {{ .Values.grafana.sidecar.datasources.searchNamespace }}
+{{- end }}
+{{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus/prometheus.yaml packages/rancher-monitoring/charts/templates/prometheus/prometheus.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus/prometheus.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus/prometheus.yaml
@@ -32,7 +32,7 @@
 {{ toYaml .Values.prometheus.prometheusSpec.apiserverConfig | indent 4}}
 {{- end }}
 {{- if .Values.prometheus.prometheusSpec.image }}
-  baseImage: {{ .Values.prometheus.prometheusSpec.image.repository }}
+  baseImage: {{ template "system_default_registry" . }}{{ .Values.prometheus.prometheusSpec.image.repository }}
   version: {{ .Values.prometheus.prometheusSpec.image.tag }}
 {{- end }}
 {{- if .Values.prometheus.prometheusSpec.externalLabels }}
@@ -54,7 +54,10 @@
 {{- else if and .Values.prometheus.ingress.enabled .Values.prometheus.ingress.hosts }}
   externalUrl: "http://{{ tpl (index .Values.prometheus.ingress.hosts 0) . }}{{ .Values.prometheus.prometheusSpec.routePrefix }}"
 {{- else }}
-  externalUrl: http://{{ template "prometheus-operator.fullname" . }}-prometheus.{{ template "prometheus-operator.namespace" . }}:{{ .Values.prometheus.service.port }}
+  externalUrl: {{ template "prometheus_proxy_url" . }}
+{{- end }}
+{{- if .Values.prometheus.prometheusSpec.ignoreNamespaceSelectors }}
+  ignoreNamespaceSelectors: {{ .Values.prometheus.prometheusSpec.ignoreNamespaceSelectors }}
 {{- end }}
 {{- if .Values.prometheus.prometheusSpec.nodeSelector }}
   nodeSelector:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus/rules/etcd.yaml packages/rancher-monitoring/charts/templates/prometheus/rules/etcd.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus/rules/etcd.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus/rules/etcd.yaml
@@ -4,7 +4,8 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.10.0-0" $kubeTargetVersion) (semverCompare "<1.14.0-0" $kubeTargetVersion) .Values.defaultRules.create .Values.kubeEtcd.enabled .Values.defaultRules.rules.etcd }}
+{{- if and (semverCompare ">=1.10.0-0" $kubeTargetVersion) (semverCompare "<1.14.0-0" $kubeTargetVersion) .Values.defaultRules.create .Values.defaultRules.rules.etcd }}
+{{- if (include "exporter.kubeEtcd.enabled" .)}}
 apiVersion: monitoring.coreos.com/v1
 kind: PrometheusRule
 metadata:
@@ -152,4 +153,5 @@
       for: 10m
       labels:
         severity: warning
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus/rules/kube-scheduler.rules.yaml packages/rancher-monitoring/charts/templates/prometheus/rules/kube-scheduler.rules.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus/rules/kube-scheduler.rules.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus/rules/kube-scheduler.rules.yaml
@@ -4,7 +4,8 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.10.0-0" $kubeTargetVersion) (semverCompare "<1.14.0-0" $kubeTargetVersion) .Values.defaultRules.create .Values.kubeScheduler.enabled .Values.defaultRules.rules.kubeScheduler }}
+{{- if and (semverCompare ">=1.10.0-0" $kubeTargetVersion) (semverCompare "<1.14.0-0" $kubeTargetVersion) .Values.defaultRules.create .Values.defaultRules.rules.kubeScheduler }}
+{{- if (include "exporter.kubeScheduler.enabled" .)}}
 apiVersion: monitoring.coreos.com/v1
 kind: PrometheusRule
 metadata:
@@ -60,4 +61,5 @@
       labels:
         quantile: '0.5'
       record: cluster_quantile:scheduler_binding_latency:histogram_quantile
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus/rules/kubernetes-absent.yaml packages/rancher-monitoring/charts/templates/prometheus/rules/kubernetes-absent.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus/rules/kubernetes-absent.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus/rules/kubernetes-absent.yaml
@@ -58,7 +58,7 @@
       labels:
         severity: critical
 {{- end }}
-{{- if .Values.kubeControllerManager.enabled }}
+{{- if (include "exporter.kubeControllerManager.enabled" .)}}
     - alert: KubeControllerManagerDown
       annotations:
         message: KubeControllerManager has disappeared from Prometheus target discovery.
@@ -68,7 +68,7 @@
       labels:
         severity: critical
 {{- end }}
-{{- if .Values.kubeScheduler.enabled }}
+{{- if (include "exporter.kubeScheduler.enabled" .)}}
     - alert: KubeSchedulerDown
       annotations:
         message: KubeScheduler has disappeared from Prometheus target discovery.
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus/rules-1.14/etcd.yaml packages/rancher-monitoring/charts/templates/prometheus/rules-1.14/etcd.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus/rules-1.14/etcd.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus/rules-1.14/etcd.yaml
@@ -4,7 +4,8 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.defaultRules.create .Values.kubeEtcd.enabled .Values.defaultRules.rules.etcd }}
+{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.defaultRules.create .Values.defaultRules.rules.etcd }}
+{{- if (include "exporter.kubeEtcd.enabled" .)}}
 apiVersion: monitoring.coreos.com/v1
 kind: PrometheusRule
 metadata:
@@ -152,4 +153,5 @@
       for: 10m
       labels:
         severity: warning
-{{- end }}
\ No newline at end of file
+{{- end }}
+{{- end }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus/rules-1.14/kube-scheduler.rules.yaml packages/rancher-monitoring/charts/templates/prometheus/rules-1.14/kube-scheduler.rules.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus/rules-1.14/kube-scheduler.rules.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus/rules-1.14/kube-scheduler.rules.yaml
@@ -4,7 +4,8 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.defaultRules.create .Values.kubeScheduler.enabled .Values.defaultRules.rules.kubeScheduler }}
+{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.defaultRules.create .Values.defaultRules.rules.kubeScheduler }}
+{{- if (include "exporter.kubeScheduler.enabled" .)}}
 apiVersion: monitoring.coreos.com/v1
 kind: PrometheusRule
 metadata:
@@ -60,4 +61,5 @@
       labels:
         quantile: '0.5'
       record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus/rules-1.14/kubernetes-system-controller-manager.yaml packages/rancher-monitoring/charts/templates/prometheus/rules-1.14/kubernetes-system-controller-manager.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus/rules-1.14/kubernetes-system-controller-manager.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus/rules-1.14/kubernetes-system-controller-manager.yaml
@@ -4,7 +4,8 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.defaultRules.create .Values.kubeControllerManager.enabled }}
+{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.defaultRules.create }}
+{{- if (include "exporter.kubeControllerManager.enabled" .)}}
 apiVersion: monitoring.coreos.com/v1
 kind: PrometheusRule
 metadata:
@@ -24,7 +25,7 @@
   groups:
   - name: kubernetes-system-controller-manager
     rules:
-{{- if .Values.kubeControllerManager.enabled }}
+{{- if (include "exporter.kubeControllerManager.enabled" .)}}
     - alert: KubeControllerManagerDown
       annotations:
         message: KubeControllerManager has disappeared from Prometheus target discovery.
@@ -34,4 +35,5 @@
       labels:
         severity: critical
 {{- end }}
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus/rules-1.14/kubernetes-system-scheduler.yaml packages/rancher-monitoring/charts/templates/prometheus/rules-1.14/kubernetes-system-scheduler.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus/rules-1.14/kubernetes-system-scheduler.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus/rules-1.14/kubernetes-system-scheduler.yaml
@@ -4,7 +4,8 @@
 https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
 */ -}}
 {{- $kubeTargetVersion := default .Capabilities.KubeVersion.GitVersion .Values.kubeTargetVersionOverride }}
-{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.defaultRules.create .Values.kubeScheduler.enabled .Values.defaultRules.rules.kubeScheduler }}
+{{- if and (semverCompare ">=1.14.0-0" $kubeTargetVersion) (semverCompare "<9.9.9-9" $kubeTargetVersion) .Values.defaultRules.create .Values.defaultRules.rules.kubeScheduler }}
+{{- if (include "exporter.kubeScheduler.enabled" .)}}
 apiVersion: monitoring.coreos.com/v1
 kind: PrometheusRule
 metadata:
@@ -24,7 +25,7 @@
   groups:
   - name: kubernetes-system-scheduler
     rules:
-{{- if .Values.kubeScheduler.enabled }}
+{{- if (include "exporter.kubeScheduler.enabled" .)}}
     - alert: KubeSchedulerDown
       annotations:
         message: KubeScheduler has disappeared from Prometheus target discovery.
@@ -34,4 +35,5 @@
       labels:
         severity: critical
 {{- end }}
+{{- end }}
 {{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus-operator/admission-webhooks/job-patch/job-createSecret.yaml packages/rancher-monitoring/charts/templates/prometheus-operator/admission-webhooks/job-patch/job-createSecret.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus-operator/admission-webhooks/job-patch/job-createSecret.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus-operator/admission-webhooks/job-patch/job-createSecret.yaml
@@ -31,7 +31,7 @@
       {{- end }}
       containers:
         - name: create
-          image: {{ .Values.prometheusOperator.admissionWebhooks.patch.image.repository }}:{{ .Values.prometheusOperator.admissionWebhooks.patch.image.tag }}
+          image: {{ template "system_default_registry" . }}{{ .Values.prometheusOperator.admissionWebhooks.patch.image.repository }}:{{ .Values.prometheusOperator.admissionWebhooks.patch.image.tag }}
           imagePullPolicy: {{ .Values.prometheusOperator.admissionWebhooks.patch.image.pullPolicy }}
           args:
             - create
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus-operator/admission-webhooks/job-patch/job-patchWebhook.yaml packages/rancher-monitoring/charts/templates/prometheus-operator/admission-webhooks/job-patch/job-patchWebhook.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus-operator/admission-webhooks/job-patch/job-patchWebhook.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus-operator/admission-webhooks/job-patch/job-patchWebhook.yaml
@@ -31,7 +31,7 @@
       {{- end }}
       containers:
         - name: patch
-          image: {{ .Values.prometheusOperator.admissionWebhooks.patch.image.repository }}:{{ .Values.prometheusOperator.admissionWebhooks.patch.image.tag }}
+          image: {{ template "system_default_registry" . }}{{ .Values.prometheusOperator.admissionWebhooks.patch.image.repository }}:{{ .Values.prometheusOperator.admissionWebhooks.patch.image.tag }}
           imagePullPolicy: {{ .Values.prometheusOperator.admissionWebhooks.patch.image.pullPolicy }}
           args:
             - patch
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus-operator/cleanup-crds.yaml packages/rancher-monitoring/charts/templates/prometheus-operator/cleanup-crds.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus-operator/cleanup-crds.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus-operator/cleanup-crds.yaml
@@ -1,45 +0,0 @@
-{{- if and .Values.prometheusOperator.enabled .Values.prometheusOperator.cleanupCustomResource }}
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: {{ template "prometheus-operator.fullname" . }}-operator-cleanup
-  namespace: {{ template "prometheus-operator.namespace" . }}
-  annotations:
-    "helm.sh/hook": pre-delete
-    "helm.sh/hook-weight": "3"
-    "helm.sh/hook-delete-policy": hook-succeeded
-  labels:
-    app: {{ template "prometheus-operator.name" . }}-operator
-{{ include "prometheus-operator.labels" . | indent 4 }}
-spec:
-  template:
-    metadata:
-      name: {{ template "prometheus-operator.fullname" . }}-operator-cleanup
-      labels:
-        app: {{ template "prometheus-operator.name" . }}-operator
-{{ include "prometheus-operator.labels" . | indent 8 }}
-    spec:
-    {{- if .Values.global.rbac.create }}
-      serviceAccountName: {{ template "prometheus-operator.operator.serviceAccountName" . }}
-    {{- end }}
-      containers:
-        - name: kubectl
-          image: "{{ .Values.prometheusOperator.hyperkubeImage.repository }}:{{ .Values.prometheusOperator.hyperkubeImage.tag }}"
-          imagePullPolicy: "{{ .Values.prometheusOperator.hyperkubeImage.pullPolicy }}"
-          command:
-          - /bin/sh
-          - -c
-          - >
-              kubectl delete alertmanager   --all;
-              kubectl delete prometheus     --all;
-              kubectl delete prometheusrule --all;
-              kubectl delete servicemonitor --all;
-              sleep 10;
-              kubectl delete crd alertmanagers.monitoring.coreos.com;
-              kubectl delete crd prometheuses.monitoring.coreos.com;
-              kubectl delete crd prometheusrules.monitoring.coreos.com;
-              kubectl delete crd servicemonitors.monitoring.coreos.com;
-              kubectl delete crd podmonitors.monitoring.coreos.com;
-              kubectl delete crd thanosrulers.monitoring.coreos.com;
-      restartPolicy: OnFailure
-{{- end }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus-operator/clusterrole.yaml packages/rancher-monitoring/charts/templates/prometheus-operator/clusterrole.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus-operator/clusterrole.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus-operator/clusterrole.yaml
@@ -7,7 +7,7 @@
     app: {{ template "prometheus-operator.name" . }}-operator
 {{ include "prometheus-operator.labels" . | indent 4 }}
 rules:
-{{- if or .Values.prometheusOperator.manageCrds .Values.prometheusOperator.cleanupCustomResource }}
+{{- if .Values.prometheusOperator.manageCrds }}
 - apiGroups:
   - apiextensions.k8s.io
   resources:
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus-operator/crds.yaml packages/rancher-monitoring/charts/templates/prometheus-operator/crds.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus-operator/crds.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus-operator/crds.yaml
@@ -1,6 +0,0 @@
-{{- if and .Values.prometheusOperator.enabled .Values.prometheusOperator.createCustomResource -}}
-{{- range $path, $bytes := .Files.Glob "crds/*.yaml" }}
-{{ $.Files.Get $path }}
----
-{{- end }}
-{{- end }}
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/prometheus-operator/deployment.yaml packages/rancher-monitoring/charts/templates/prometheus-operator/deployment.yaml
--- packages/rancher-monitoring/charts-original/templates/prometheus-operator/deployment.yaml
+++ packages/rancher-monitoring/charts/templates/prometheus-operator/deployment.yaml
@@ -32,7 +32,7 @@
     {{- end }}
       containers:
         - name: {{ template "prometheus-operator.name" . }}
-          image: "{{ .Values.prometheusOperator.image.repository }}:{{ .Values.prometheusOperator.image.tag }}"
+          image: "{{ template "system_default_registry" . }}{{ .Values.prometheusOperator.image.repository }}:{{ .Values.prometheusOperator.image.tag }}"
           imagePullPolicy: "{{ .Values.prometheusOperator.image.pullPolicy }}"
           args:
           {{- if semverCompare "< v0.39.0" .Values.prometheusOperator.image.tag }}
@@ -59,8 +59,8 @@
           {{- end }}
             - --logtostderr=true
             - --localhost=127.0.0.1
-            - --prometheus-config-reloader={{ .Values.prometheusOperator.prometheusConfigReloaderImage.repository }}:{{ .Values.prometheusOperator.prometheusConfigReloaderImage.tag }}
-            - --config-reloader-image={{ .Values.prometheusOperator.configmapReloadImage.repository }}:{{ .Values.prometheusOperator.configmapReloadImage.tag }}
+            - --prometheus-config-reloader={{ template "system_default_registry" . }}{{ .Values.prometheusOperator.prometheusConfigReloaderImage.repository }}:{{ .Values.prometheusOperator.prometheusConfigReloaderImage.tag }}
+            - --config-reloader-image={{ template "system_default_registry" . }}{{ .Values.prometheusOperator.configmapReloadImage.repository }}:{{ .Values.prometheusOperator.configmapReloadImage.tag }}
             - --config-reloader-cpu={{ .Values.prometheusOperator.configReloaderCpu }}
             - --config-reloader-memory={{ .Values.prometheusOperator.configReloaderMemory }}
           ports:
@@ -73,7 +73,7 @@
             readOnlyRootFilesystem: true
         {{- if .Values.prometheusOperator.tlsProxy.enabled }}
         - name: tls-proxy
-          image: {{ .Values.prometheusOperator.tlsProxy.image.repository }}:{{ .Values.prometheusOperator.tlsProxy.image.tag }}
+          image: {{ template "system_default_registry" . }}{{ .Values.prometheusOperator.tlsProxy.image.repository }}:{{ .Values.prometheusOperator.tlsProxy.image.tag }}
           imagePullPolicy: {{ .Values.prometheusOperator.tlsProxy.image.pullPolicy }}
           args:
             - server
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/templates/rancher-monitoring/clusterrole.yaml packages/rancher-monitoring/charts/templates/rancher-monitoring/clusterrole.yaml
--- packages/rancher-monitoring/charts-original/templates/rancher-monitoring/clusterrole.yaml
+++ packages/rancher-monitoring/charts/templates/rancher-monitoring/clusterrole.yaml
@@ -0,0 +1,148 @@
+{{- if and .Values.global.rbac.create .Values.global.rbac.userRoles.create }}
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: monitoring-admin
+  labels: {{ include "prometheus-operator.labels" . | nindent 4 }}
+  {{- if .Values.global.rbac.userRoles.aggregateToDefaultRoles }}
+    rbac.authorization.k8s.io/aggregate-to-admin: "true"
+  {{- end }}
+rules:
+- apiGroups:
+  - monitoring.coreos.com
+  resources:
+  - alertmanagers
+  - prometheuses
+  - prometheuses/finalizers
+  - alertmanagers/finalizers
+  verbs:
+  - 'get'
+  - 'list'
+  - 'watch'
+- apiGroups:
+  - monitoring.coreos.com
+  resources:
+  - thanosrulers
+  - thanosrulers/finalizers
+  - servicemonitors
+  - podmonitors
+  - prometheusrules
+  - podmonitors
+  verbs:
+  - '*'
+- apiGroups:
+  - ""
+  resources:
+  - configmaps
+  - secrets
+  verbs:
+  - '*'
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: monitoring-edit
+  labels: {{ include "prometheus-operator.labels" . | nindent 4 }}
+  {{- if .Values.global.rbac.userRoles.aggregateToDefaultRoles }}
+    rbac.authorization.k8s.io/aggregate-to-edit: "true"
+  {{- end }}
+rules:
+rules:
+- apiGroups:
+  - monitoring.coreos.com
+  resources:
+  - alertmanagers
+  - prometheuses
+  - prometheuses/finalizers
+  - alertmanagers/finalizers
+  verbs:
+  - 'get'
+  - 'list'
+  - 'watch'
+- apiGroups:
+  - monitoring.coreos.com
+  resources:
+  - thanosrulers
+  - thanosrulers/finalizers
+  - servicemonitors
+  - podmonitors
+  - prometheusrules
+  - podmonitors
+  verbs:
+  - '*'
+- apiGroups:
+  - ""
+  resources:
+  - configmaps
+  - secrets
+  verbs:
+  - '*'
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: monitoring-view
+  labels: {{ include "prometheus-operator.labels" . | nindent 4 }}
+  {{- if .Values.global.rbac.userRoles.aggregateToDefaultRoles }}
+    rbac.authorization.k8s.io/aggregate-to-view: "true"
+  {{- end }}
+rules:
+- apiGroups:
+  - monitoring.coreos.com
+  resources:
+  - alertmanagers
+  - prometheuses
+  - prometheuses/finalizers
+  - alertmanagers/finalizers
+  - thanosrulers
+  - thanosrulers/finalizers
+  - servicemonitors
+  - podmonitors
+  - prometheusrules
+  - podmonitors
+  verbs:
+  - 'get'
+  - 'list'
+  - 'watch'
+- apiGroups:
+  - ""
+  resources:
+  - configmaps
+  - secrets
+  verbs:
+  - 'get'
+  - 'list'
+  - 'watch'
+{{- if .Values.grafana.enabled }}
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: grafana-config-edit
+  labels: {{ include "prometheus-operator.labels" . | nindent 4 }}
+rules:
+- apiGroups:
+  - ""
+  resources:
+  - configmaps
+  - secrets
+  verbs:
+  - '*'
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: grafana-config-view
+  labels: {{ include "prometheus-operator.labels" . | nindent 4 }}
+rules:
+- apiGroups:
+  - ""
+  resources:
+  - configmaps
+  - secrets
+  verbs:
+  - 'get'
+  - 'list'
+  - 'watch'
+{{- end }}
+{{- end }}
\ No newline at end of file
diff -x '*.tgz' -x '*.lock' -uNr packages/rancher-monitoring/charts-original/values.yaml packages/rancher-monitoring/charts/values.yaml
--- packages/rancher-monitoring/charts-original/values.yaml
+++ packages/rancher-monitoring/charts/values.yaml
@@ -2,13 +2,271 @@
 # This is a YAML-formatted file.
 # Declare variables to be passed into your templates.
 
+# Rancher Monitoring Configuration
+
+## Configuration for prometheus-adapter
+## ref: https://github.com/helm/charts/tree/master/stable/prometheus-adapter
+##
+prometheus-adapter:
+  enabled: true
+  prometheus:
+    # Change this if you change the namespaceOverride or nameOverride of prometheus-operator
+    url: http://rancher-monitoring-prometheus.cattle-monitoring-system.svc
+    port: 9090
+  image:
+    repository: rancher/directxman12-k8s-prometheus-adapter-amd64
+    tag: v0.6.0
+    pullPolicy: IfNotPresent
+    pullSecrets: {}
+
+## RKE PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/master/packages/rancher-pushprox
+##
+rkeControllerManager:
+  enabled: false
+  metricsPort: 10252
+  component: kube-controller-manager
+  clients:
+    port: 10011
+    nodeSelector:
+      node-role.kubernetes.io/controlplane: "true"
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+rkeScheduler:
+  enabled: false
+  metricsPort: 10251
+  component: kube-scheduler
+  clients:
+    port: 10012
+    nodeSelector:
+      node-role.kubernetes.io/controlplane: "true"
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+rkeProxy:
+  enabled: false
+  metricsPort: 10249
+  component: kube-proxy
+  clients:
+    port: 10013
+    useLocalhost: true
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+rkeEtcd:
+  enabled: false
+  metricsPort: 2379
+  component: kube-etcd
+  clients:
+    port: 10014
+    https:
+      enabled: true
+      certDir: /etc/kubernetes/ssl
+      certFile: kube-etcd-*.pem
+      keyFile: kube-etcd-*-key.pem
+      caCertFile: kube-ca.pem
+    nodeSelector:
+      node-role.kubernetes.io/etcd: "true"
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+## k3s PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/master/packages/rancher-pushprox
+##
+k3sControllerManager:
+  enabled: false
+  metricsPort: 10252
+  component: kube-controller-manager
+  clients:
+    port: 10011
+    nodeSelector:
+      node-role.kubernetes.io/master: "true"
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+k3sScheduler:
+  enabled: false
+  metricsPort: 10251
+  component: kube-scheduler
+  clients:
+    port: 10012
+    nodeSelector:
+      node-role.kubernetes.io/master: "true"
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+k3sProxy:
+  enabled: false
+  metricsPort: 10249
+  component: kube-proxy
+  clients:
+    port: 10013
+    useLocalhost: true
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+## KubeADM PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/master/packages/rancher-pushprox
+##
+kubeAdmControllerManager:
+  enabled: false
+  metricsPort: 10257
+  component: kube-controller-manager
+  clients:
+    port: 10011
+    useLocalhost: true
+    https:
+      enabled: true
+      useServiceAccountCredentials: true
+      insecureSkipVerify: true
+    nodeSelector:
+      node-role.kubernetes.io/master: ""
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+kubeAdmScheduler:
+  enabled: false
+  metricsPort: 10259
+  component: kube-scheduler
+  clients:
+    port: 10012
+    useLocalhost: true
+    https:
+      enabled: true
+      useServiceAccountCredentials: true
+      insecureSkipVerify: true
+    nodeSelector:
+      node-role.kubernetes.io/master: ""
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+kubeAdmProxy:
+  enabled: false
+  metricsPort: 10249
+  component: kube-proxy
+  clients:
+    port: 10013
+    useLocalhost: true
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+kubeAdmEtcd:
+  enabled: false
+  metricsPort: 2381
+  component: kube-etcd
+  clients:
+    port: 10014
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/master: ""
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+## rke2 PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/master/packages/rancher-pushprox
+##
+rke2ControllerManager:
+  enabled: false
+  metricsPort: 10252
+  component: kube-controller-manager
+  clients:
+    port: 10011
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/master: "true"
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+
+rke2Scheduler:
+  enabled: false
+  metricsPort: 10251
+  component: kube-scheduler
+  clients:
+    port: 10012
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/master: "true"
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+
+rke2Proxy:
+  enabled: false
+  metricsPort: 10249
+  component: kube-proxy
+  clients:
+    port: 10013
+    useLocalhost: true
+  tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+rke2Etcd:
+  enabled: false
+  metricsPort: 2381
+  component: kube-etcd
+  clients:
+    port: 10014
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/etcd: "true"
+    tolerations:
+      - effect: "NoSchedule"
+        key: node-role.kubernetes.io/master
+        operator: "Equal"
+
+# Prometheus Operator Configuration
+
 ## Provide a name in place of prometheus-operator for `app:` labels
+## NOTE: If you change this value, you must update the prometheus-adapter.prometheus.url
 ##
-nameOverride: ""
+nameOverride: "rancher-monitoring"
 
 ## Override the deployment namespace
+## NOTE: If you change this value, you must update the prometheus-adapter.prometheus.url
 ##
-namespaceOverride: ""
+namespaceOverride: "cattle-monitoring-system"
 
 ## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6
 ##
@@ -76,8 +334,21 @@
 
 ##
 global:
+  cattle:
+    systemDefaultRegistry: ""
+    url: ""
+    clusterId: ""
   rbac:
+    ## Create RBAC resources for ServiceAccounts and users 
+    ##
     create: true
+
+    userRoles:
+      ## Create default user ClusterRoles to allow users to interact with Prometheus CRs, ConfigMaps, and Secrets
+      create: true
+      ## Aggregate default user ClusterRoles into default k8s ClusterRoles
+      aggregateToDefaultRoles: true
+
     pspEnabled: true
     pspAnnotations: {}
       ## Specify pod annotations
@@ -130,6 +401,22 @@
   ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
   ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
   ##
+  ## Example Slack Config
+  ## config:
+  ##   route:
+  ##     group_by: ['job']
+  ##     group_wait: 30s
+  ##     group_interval: 5m
+  ##     repeat_interval: 3h
+  ##     receiver: 'slack-notifications'
+  ##   receivers:
+  ##   - name: 'slack-notifications'
+  ##     slack_configs:
+  ##     - send_resolved: true
+  ##       text: '{{ template "slack.rancher.text" . }}'
+  ##       api_url: <slack-webhook-url-here>
+  ##   templates:
+  ##   - /etc/alertmanager/config/*.tmpl
   config:
     global:
       resolve_timeout: 5m
@@ -145,6 +432,8 @@
         receiver: 'null'
     receivers:
     - name: 'null'
+    templates:
+    - /etc/alertmanager/config/*.tmpl
 
   ## Pass the Alertmanager configuration directives through Helm's templating
   ## engine. If the Alertmanager configuration contains Alertmanager templates,
@@ -160,25 +449,76 @@
   ## ref: https://prometheus.io/docs/alerting/notifications/
   ##      https://prometheus.io/docs/alerting/notification_examples/
   ##
-  templateFiles: {}
-  #
-  ## An example template:
-  #   template_1.tmpl: |-
-  #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
-  #
-  #       {{ define "slack.myorg.text" }}
-  #       {{- $root := . -}}
-  #       {{ range .Alerts }}
-  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
-  #         *Cluster:*  {{ template "cluster" $root }}
-  #         *Description:* {{ .Annotations.description }}
-  #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
-  #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
-  #         *Details:*
-  #           {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
-  #           {{ end }}
-  #       {{ end }}
-  #       {{ end }}
+  templateFiles:
+    rancher_defaults.tmpl: |-
+        {{- define "slack.rancher.text" -}}
+        {{ template "rancher.text_multiple" . }}
+        {{- end -}}
+
+        {{- define "rancher.text_multiple" -}}
+        *[GROUP - Details]*
+        One or more alarms in this group have triggered a notification.
+
+        {{- if gt (len .GroupLabels.Values) 0 }}
+        *Group Labels:*
+          {{- range .GroupLabels.SortedPairs }}
+          • *{{ .Name }}:* `{{ .Value }}`
+          {{- end }}
+        {{- end }}
+        {{- if .ExternalURL }}
+        *Link to AlertManager:* {{ .ExternalURL }}
+        {{- end }}
+
+        {{- range .Alerts }}
+        {{ template "rancher.text_single" . }}
+        {{- end }}
+        {{- end -}}
+
+        {{- define "rancher.text_single" -}}
+        {{- if .Labels.alertname }}
+        *[ALERT - {{ .Labels.alertname }}]*
+        {{- else }}
+        *[ALERT]*
+        {{- end }}
+        {{- if .Labels.severity }}
+        *Severity:* `{{ .Labels.severity }}`
+        {{- end }}
+        {{- if .Labels.cluster }}
+        *Cluster:*  {{ .Labels.cluster }}
+        {{- end }}
+        {{- if .Annotations.summary }}
+        *Summary:* {{ .Annotations.summary }}
+        {{- end }}
+        {{- if .Annotations.message }}
+        *Message:* {{ .Annotations.message }}
+        {{- end }}
+        {{- if .Annotations.description }}
+        *Description:* {{ .Annotations.description }}
+        {{- end }}
+        {{- if .Annotations.runbook_url }}
+        *Runbook URL:* <{{ .Annotations.runbook_url }}|:spiral_note_pad:>
+        {{- end }}
+        {{- with .Labels }}
+        {{- with .Remove (stringSlice "alertname" "severity" "cluster") }}
+        {{- if gt (len .) 0 }}
+        *Additional Labels:*
+          {{- range .SortedPairs }}
+          • *{{ .Name }}:* `{{ .Value }}`
+          {{- end }}
+        {{- end }}
+        {{- end }}
+        {{- end }}
+        {{- with .Annotations }}
+        {{- with .Remove (stringSlice "summary" "message" "description" "runbook_url") }}
+        {{- if gt (len .) 0 }}
+        *Additional Annotations:*
+          {{- range .SortedPairs }}
+          • *{{ .Name }}:* `{{ .Value }}`
+          {{- end }}
+        {{- end }}
+        {{- end }}
+        {{- end }}
+        {{- end -}}
 
   ingress:
     enabled: false
@@ -208,6 +546,21 @@
   ## Configuration for Alertmanager secret
   ##
   secret:
+
+    # Should the Alertmanager Config Secret be cleaned up on an uninstall?
+    # This is set to false by default to prevent the loss of alerting configuration on an uninstall
+    # Only used Alertmanager is deployed and alertmanager.alertmanagerSpec.useExistingSecret=false
+    #
+    cleanupOnUninstall: false
+
+    # The image used to manage the Alertmanager Config Secret's lifecycle
+    # Only used Alertmanager is deployed and alertmanager.alertmanagerSpec.useExistingSecret=false
+    #
+    image:
+      repository: rancher/rancher-agent
+      tag: v2.4.8
+      pullPolicy: IfNotPresent
+
     annotations: {}
 
   ## Configuration for creating an Ingress that will map to each Alertmanager replica service
@@ -334,7 +687,7 @@
     ## Image of Alertmanager
     ##
     image:
-      repository: quay.io/prometheus/alertmanager
+      repository: rancher/prom-alertmanager
       tag: v0.20.0
 
     ## If true then the user will be responsible to provide a secret with alertmanager configuration
@@ -409,9 +762,13 @@
     ## Define resources requests and limits for single Pods.
     ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
     ##
-    resources: {}
-    # requests:
-    #   memory: 400Mi
+    resources:
+      limits:
+        memory: 500Mi
+        cpu: 1000m
+      requests:
+        memory: 100Mi
+        cpu: 100m
 
     ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
     ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
@@ -486,6 +843,9 @@
   enabled: true
   namespaceOverride: ""
 
+  deploymentStrategy:
+    type: Recreate
+
   ## Deploy default dashboards.
   ##
   defaultDashboardsEnabled: true
@@ -529,6 +889,7 @@
     dashboards:
       enabled: true
       label: grafana_dashboard
+      searchNamespace: grafana-dashboards
 
       ## Annotations for Grafana dashboard configmaps
       ##
@@ -547,6 +908,7 @@
       ## ref: https://git.io/fjaBS
       createPrometheusReplicasDatasources: false
       label: grafana_datasource
+      searchNamespace: grafana-datasources
 
   extraConfigmapMounts: []
   # - name: certs-configmap
@@ -574,6 +936,19 @@
   ##
   service:
     portName: service
+    ## Port for Grafana Service to listen on
+    ##
+    port: 80
+    ## To be used with a proxy extraContainer port
+    ##
+    targetPort: 3000
+    ## Port to expose on each node
+    ## Only used if service.type is 'NodePort'
+    ##
+    nodePort: 30950
+    ## Service type
+    ##
+    type: ClusterIP
 
   ## If true, create a serviceMonitor for grafana
   ##
@@ -599,6 +974,14 @@
     #   targetLabel: nodename
     #   replacement: $1
     #   action: replace
+  
+  resources:
+    limits:
+      memory: 200Mi
+      cpu: 200m
+    requests:
+      memory: 100Mi
+      cpu: 100m
 
 ## Component scraping the kube api server
 ##
@@ -755,7 +1138,7 @@
 ## Component scraping the kube controller manager
 ##
 kubeControllerManager:
-  enabled: true
+  enabled: false
 
   ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
   ##
@@ -888,7 +1271,7 @@
 ## Component scraping etcd
 ##
 kubeEtcd:
-  enabled: true
+  enabled: false
 
   ## If your etcd is not deployed as a pod, specify IPs it can be found on
   ##
@@ -948,7 +1331,7 @@
 ## Component scraping kube scheduler
 ##
 kubeScheduler:
-  enabled: true
+  enabled: false
 
   ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
   ##
@@ -1001,7 +1384,7 @@
 ## Component scraping kube proxy
 ##
 kubeProxy:
-  enabled: true
+  enabled: false
 
   ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
   ##
@@ -1075,6 +1458,13 @@
     create: true
   podSecurityPolicy:
     enabled: true
+  resources:
+    limits:
+      cpu: 100m
+      memory: 200Mi
+    requests:
+      cpu: 100m
+      memory: 130Mi
 
 ## Deploy node exporter as a daemonset to all nodes
 ##
@@ -1124,6 +1514,16 @@
   extraArgs:
     - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
     - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
+  service:
+    port: 9796
+    targetPort: 9796
+  resources:
+    limits:
+      cpu: 200m
+      memory: 50Mi
+    requests:
+      cpu: 100m
+      memory: 30Mi
 
 ## Manages Prometheus and Alertmanager components
 ##
@@ -1137,7 +1537,7 @@
   tlsProxy:
     enabled: true
     image:
-      repository: squareup/ghostunnel
+      repository: rancher/squareup-ghostunnel
       tag: v1.5.2
       pullPolicy: IfNotPresent
     resources: {}
@@ -1154,7 +1554,7 @@
     patch:
       enabled: true
       image:
-        repository: jettech/kube-webhook-certgen
+        repository: rancher/jettech-kube-webhook-certgen
         tag: v1.2.1
         pullPolicy: IfNotPresent
       resources: {}
@@ -1280,13 +1680,13 @@
 
   ## Resource limits & requests
   ##
-  resources: {}
-  # limits:
-  #   cpu: 200m
-  #   memory: 200Mi
-  # requests:
-  #   cpu: 100m
-  #   memory: 100Mi
+  resources:
+    limits:
+      cpu: 200m
+      memory: 500Mi
+    requests:
+      cpu: 100m
+      memory: 100Mi
 
   # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
   # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
@@ -1330,20 +1730,20 @@
   ## Prometheus-operator image
   ##
   image:
-    repository: quay.io/coreos/prometheus-operator
+    repository: rancher/coreos-prometheus-operator
     tag: v0.38.1
     pullPolicy: IfNotPresent
 
   ## Configmap-reload image to use for reloading configmaps
   ##
   configmapReloadImage:
-    repository: docker.io/jimmidyson/configmap-reload
+    repository: rancher/jimmidyson-configmap-reload
     tag: v0.3.0
 
   ## Prometheus-config-reloader image to use for config and rule reloading
   ##
   prometheusConfigReloaderImage:
-    repository: quay.io/coreos/prometheus-config-reloader
+    repository: rancher/coreos-prometheus-config-reloader
     tag: v0.38.1
 
   ## Set the prometheus config reloader side-car CPU limit
@@ -1354,13 +1754,6 @@
   ##
   configReloaderMemory: 25Mi
 
-  ## Hyperkube image to use when cleaning up
-  ##
-  hyperkubeImage:
-    repository: k8s.gcr.io/hyperkube
-    tag: v1.16.12
-    pullPolicy: IfNotPresent
-
 ## Deploy a Prometheus instance
 ##
 prometheus:
@@ -1577,7 +1970,7 @@
     ## Image of Prometheus.
     ##
     image:
-      repository: quay.io/prometheus/prometheus
+      repository: rancher/prom-prometheus
       tag: v2.18.1
 
     ## Tolerations for use with node taints
@@ -1628,6 +2021,11 @@
     ##
     externalUrl: ""
 
+    ## Ignore NamespaceSelector settings from the PodMonitor and ServiceMonitor configs
+    ## If true, PodMonitors and ServiceMonitors can only discover Pods and Services within the namespace they are deployed into
+    ##
+    ignoreNamespaceSelectors: true
+
     ## Define which Nodes the Pods are scheduled on.
     ## ref: https://kubernetes.io/docs/user-guide/node-selection/
     ##
@@ -1660,7 +2058,7 @@
     ## prometheus resource to be created with selectors based on values in the helm deployment,
     ## which will also match the PrometheusRule resources created
     ##
-    ruleSelectorNilUsesHelmValues: true
+    ruleSelectorNilUsesHelmValues: false
 
     ## PrometheusRules to be selected for target discovery.
     ## If {}, select all ServiceMonitors
@@ -1685,7 +2083,7 @@
     ## prometheus resource to be created with selectors based on values in the helm deployment,
     ## which will also match the servicemonitors created
     ##
-    serviceMonitorSelectorNilUsesHelmValues: true
+    serviceMonitorSelectorNilUsesHelmValues: false
 
     ## ServiceMonitors to be selected for target discovery.
     ## If {}, select all ServiceMonitors
@@ -1705,7 +2103,7 @@
     ## prometheus resource to be created with selectors based on values in the helm deployment,
     ## which will also match the podmonitors created
     ##
-    podMonitorSelectorNilUsesHelmValues: true
+    podMonitorSelectorNilUsesHelmValues: false
 
     ## PodMonitors to be selected for target discovery.
     ## If {}, select all PodMonitors
@@ -1802,9 +2200,13 @@
 
     ## Resource limits & requests
     ##
-    resources: {}
-    # requests:
-    #   memory: 400Mi
+    resources:
+      limits:
+        memory: 1500Mi
+        cpu: 1000m
+      requests:
+        memory: 750Mi
+        cpu: 750m
 
     ## Prometheus StorageSpec for persistent data
     ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
